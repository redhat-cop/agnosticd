:imagesdir: https://github.com/redhat-cop/agnosticd/tree/development/training/images/

= How to deploy a base config on Agnostic D from an OpenStack Sandbox

Assuming you’ve correctly configured your vars files, it’s time to deploy "a base config". (Make sure your virtualenv is active, your prompt should start `(openstack) `).

==== Preparing your a-base-config vars
The AgnosticD config a-base-config comes with sample vars files included for OpenStack, locally at ~/agnosticd/ansible/configs/a-base-config/sample_variables/rhel8_server_on_osp.yml. Only a few variables need changed.

1. Copy the `rhel8_server_on_osp.yml` file and call it `my_vars.yml`.
+
[source,bash]
----
$ cp configs/a-base-config/sample_variables/rhel8_server_on_osp.yml \
  ~/my_vars.yml
----

2. Edit your copy of the vars file, changing the value of guid to a valid subdomain name (For example an alphanumeric string starting with a letter). The values that need changing are identified below:
+
[source,bash]
----
$ vi ~/my_vars.yml
cloud_provider: osp                  # This var file is meant for an openstack deployment
env_type: a-base-config              # Name of the config to deploy
software_to_deploy: none             # Not deploying any software onto the environment

guid: CHANGE_ME

# Student User ID
student_name: CHANGE_ME              # Change to guid of the environment. This will be used to create a student login ID
student_password: "password"         # Customize the student password here. Keep in mind these systems may be public facing.
admin_user: CHANGE_ME                # Change to the guid of the environment
output_dir: /tmp/output_dir          # Writable working scratch directory
email: CHANGE_ME                     # User info for notifications
----

WARNING: Do not pick the same GUID as the one you got for access to the OSP cluster.

3. Execute the main AgnosticD playbook (bear in mind the path to your files, which may differ):
+
[source,bash]
----
(openstack) [agilpipp-redhat.com@bastion ansible]$ ansible-playbook main.yml \
   -e @~/my_vars.yml \ 
   -e @~/secrets.yml
----
+

NOTE: If you are having python2 Vs. Python3 issues, Add `/usr/bin/python3.6` before the ansible-playbook command. For example: `/usr/bin/python3.6 ansible-playbook ansible/main.yml -e @~/my_vars.yml -e@~/secrets.yml`

4. Finding your new servers

Congratulations!
You should now have your base config deployed succesfully.

In the logs that scrolled by you may have noticed the IP address of your new bastion server. This, and your other instance(s), have been captured in a temporary working directory, the `output_dir` set in your `~/my_vars.yml` file typically set to `/tmp/output_dir` if you have not changed it previously. This contains, amongst other files, an ssh configuration file.

Append this file to your existing ~/.ssh/config in the case you are using a sandbox instance:
+
[source,bash]
----
$ cat /tmp/output_dir/*_ssh_conf >> ~/.ssh/config
----

Now you can ssh to either of your new servers (bastion or node):
+
[source,bash]
----
$ ssh node
----

You can also log into your machines from the outside world (your laptop) if that is more confortable for you. First locate the public IP addresses of your machines:
+
[source,bash]
----
[YOUR_USER@bastion ~]$ openstack --os-cloud=${GUID}-project server list

+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| ID                                   | Name    | Status | Networks                                                | Image | Flavor  |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| 2715f0d9-51e1-4619-a97e-c841914dddf6 | node    | ACTIVE | testamaya-default-network=192.168.47.26                 |       | 2c2g30d |
| 6931bf5a-ec1e-4ac7-8477-9e96f9e14de3 | bastion | ACTIVE | testamaya-default-network=192.168.47.17, 169.47.188.156 |       | 2c2g30d |
| 947d6397-c152-4a38-9825-02f9fa50c03e | bastion | ACTIVE | 98e1-testnet-network=192.168.0.35, 169.47.191.80        |       | 2c2g30d |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
----

You can log using your `${GUID}_infra_ssh_key.pem` key file in the `/tmp/output_dir`. Please note that the key file should be created in the machine you launched the playbook from.
+
[source,bash]
---- 
sassenach:~ Cibeles$ ssh -i /tmp/output_dir/testamaya_infra_ssh_key.pem cloud-user@169.47.188.156
Last login: Thu May 28 10:49:27 2020 from 90.77.177.210
[cloud-user@bastion 0 ~]$
----

=== See your Deployment from your OSP
OpenStack has an web front end Horizon and both your secrets.yml and ~/.config/clouds.yml contain the necessary login information:

1. Find your credentials and URL (values may differ):
+
[source,bash]
----
$ cat ~/secrets.yml
osp_auth_project_domain: default
osp_auth_user_domain: default

# dynamic DNS
ddns_key_name: example_students
ddns_key_secret: very-secret
osp_cluster_dns_server: ddns01.example.com
osp_cluster_dns_zone: students.osp.example.com

osp_auth_username: "a890-user"                            (1)
osp_auth_password: "YOUR_PASSWD"                          (2)
osp_auth_url: "http://http://169.47.188.15:5000/v3"      (3)
osp_project_name: "a890-project"
osp_project_id: ca3ef1650c6940058d1dd96236fc8619"

osp_project_create: false
----

2. Browse to the Domain name or IP address in 3 osp_auth_url omitting everything from the : i.e. in my case: "http://169.47.188.15 (but you can also use the generic URL, http://horizon.red.osp.opentlc.com).

3. Authenticate with the username and password at 1 and 2 above, ie.g. for me:

image::horizon_login.png[Horizon login]

4. Select the Network Dropdown then Network Topology

image::horizon_net_menu.png[Network topology menu]

5. Here you can see your original Sandbox Instance and your new bastion and node:

image::horizon_netwk_top.png[Network topology]
