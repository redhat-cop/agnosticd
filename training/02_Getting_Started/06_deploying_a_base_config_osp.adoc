
= How to deploy a base config on Agnostic D from an OpenStack Sandbox

Assuming you’ve correctly configured your vars files, it’s time to deploy "a base config". (Make sure your virtualenv is active, your prompt should start `(openstack) `).

== Preparing your a-base-config vars
The AgnosticD config a-base-config comes with sample vars files included for OpenStack, locally at `~/agnosticd/ansible/configs/a-base-config/sample_variables/rhel8_server_on_osp.yml`. Only a few variables need changed.

1. Copy the `rhel8_server_on_osp.yml` file and call it `my_vars.yml`.
+
[source,bash]
----
$ cp configs/a-base-config/sample_variables/rhel8_server_on_osp.yml \
  ~/my_vars.yml
----

2. Edit your copy of the vars file, changing the value of guid to a valid subdomain name (For example an alphanumeric string starting with a letter). The values that need changing are identified below:
+
[source,bash]
----
$ vi ~/my_vars.yml
cloud_provider: osp                  # This var file is meant for an openstack deployment
env_type: a-base-config              # Name of the config to deploy
software_to_deploy: none             # Not deploying any software onto the environment

guid: CHANGE_ME

# Student User ID
student_name: CHANGE_ME              # Change to guid of the environment. This will be used to create a student login ID
student_password: "YOUR_PASSWD"      # Customize the student password here. Keep in mind these systems may be public facing.
admin_user: CHANGE_ME                # Change to the guid of the environment
output_dir: /tmp/output_dir          # Writable working scratch directory
email: CHANGE_ME                     # User info for notifications
----

WARNING: Do not pick the same GUID as the one you got for access to the OSP cluster.

== Deploy a base config

Execute the main AgnosticD playbook (bear in mind the path to your files, which may differ):
+
[source,bash]
----
(openstack) [agilpipp-redhat.com@bastion ansible]$ ansible-playbook main.yml \
   -e @~/my_vars.yml \ 
   -e @~/secrets.yml
----
+

NOTE: If you are having python2 Vs. Python3 issues, Add `/usr/bin/python3.6` before the ansible-playbook command. For example: `/usr/bin/python3.6 ansible-playbook ansible/main.yml -e @~/my_vars.yml -e@~/secrets.yml`

== Log into your new node(s)

Congratulations!
You should now have your base config deployed succesfully.

In the logs that scrolled by you may have noticed the IP address of your new bastion server. This, and your other instance(s), have been captured in a temporary working directory, the `output_dir` set in your `~/my_vars.yml` file typically set to `/tmp/output_dir` if you have not changed it previously. This contains, amongst other files, an ssh configuration file.

Append this file to your existing ~/.ssh/config in the case you are using a sandbox instance:
[source,bash]
----
$ cat /tmp/output_dir/*_ssh_conf >> ~/.ssh/config
----

Now you can ssh to either of your new servers (bastion or node):
[source,bash]
----
$ ssh node
----

You can also log into your machines from the outside world (your laptop) if that is more confortable for you. First locate the public IP addresses of your machines:
[source,bash]
----
[YOUR_USER@bastion ~]$ openstack --os-cloud=${GUID}-project server list

+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| ID                                   | Name    | Status | Networks                                                | Image | Flavor  |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| 2715f0d9-51e1-4619-a97e-c841914dddf6 | node    | ACTIVE | testamaya-default-network=192.168.47.26                 |       | 2c2g30d |
| 6931bf5a-ec1e-4ac7-8477-9e96f9e14de3 | bastion | ACTIVE | testamaya-default-network=192.168.47.17, 169.47.188.156 |       | 2c2g30d |
| 947d6397-c152-4a38-9825-02f9fa50c03e | bastion | ACTIVE | 98e1-testnet-network=192.168.0.35, 169.47.191.80        |       | 2c2g30d |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
----

You can log using your `${GUID}_infra_ssh_key.pem` key file in the `/tmp/output_dir`. Please note that the key file should be created in the machine you launched the playbook from.
[source,bash]
---- 
sassenach:~ Cibeles$ ssh -i /tmp/output_dir/testamaya_infra_ssh_key.pem cloud-user@169.47.188.156
Last login: Thu May 28 10:49:27 2020 from 90.77.177.210
[cloud-user@bastion 0 ~]$
----

=== See your Deployment from your OSP
OpenStack has an web front end Horizon and both your secrets.yml and ~/.config/clouds.yml contain the necessary login information:

. Find your credentials and URL (values may differ):
+
[source,bash]
----
$ cat ~/secrets.yml
osp_auth_project_domain: default
osp_auth_user_domain: default

# dynamic DNS
ddns_key_name: example_students
ddns_key_secret: very-secret
osp_cluster_dns_server: ddns01.example.com
osp_cluster_dns_zone: students.osp.example.com

osp_auth_username: "a890-user"                            (1)
osp_auth_password: "YOUR_PASSWD"                          (2)
osp_auth_url: "http://http://169.47.188.15:5000/v3"       (3)
osp_project_name: "a890-project"
osp_project_id: ca3ef1650c6940058d1dd96236fc8619"

osp_project_create: false
----

. Browse to the Domain name or IP address in 3 osp_auth_url omitting everything from the : i.e. in my case: "http://169.47.188.15 (but you can also use the generic URL, http://horizon.red.osp.opentlc.com).

. Authenticate with the username and password at 1 and 2 above, ie.g. for me:

image::../images/horizon_login.png[Horizon login]

. Select the Network Dropdown then Network Topology

image::../images/horizon_net_menu.png[Network topology menu]

. Here you can see your original Sandbox Instance and your new bastion and node:

image::../images/horizon_netwk_top.png[Network topology]

=== Other important variables

The `rhel8_server_on_ec2.yml` file is full of comments for easier understanding, but since it's also lengthy, here I'll cover some of the most important variables you may find that will allow you to rapidly customize your AgnosticD deployment:

==== Instance Variables
Instance variables will allow you to choose the operating system type and version, as well as the flavor of the osp instance. These are defined on the "instance variables section" as follows:
[source,bash]
----
rhel_image: rhel-8.1                     # Image within OSP to use for systems
node_instance_count: 1                   # The number of nodes to deploy for this environment
node_instance_image: "{{ rhel_image }}"  # Image to use for the node_instance defined in the default_vars.yml
node_instance_type: 2c2g30d              # Existing OSP flavor
node_rootfs_size: 30                     # Size of the root filesystem
----

In order to know where to pull the images from, you can simply run `openstack image list --os-cloud=GUID-project` from your bastion machine to see all the images available.
[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ openstack image list --os-cloud=a890-project |grep -i rhel
| d724a4f9-94d5-47db-8e48-3fbca378aa4f | rhel-8.0-update-3          active |
| 073088b1-9533-4301-9b75-49f9a8760c04 | rhel-8.1                   active |
| 08269fc5-1f16-43e1-abac-346b302ef780 | rhel-8.1-dnsmasq           active |
| 5d108a3c-24f9-45f1-b389-ae106553e0f1 | rhel-server-7.6-update-5   active |
| 93de52d1-2982-43e6-8481-491fc93225e9 | rhel-server-7.7            active |
| 49271e6c-2d3e-4f78-95c8-4cab3c96ac29 | rhel-server-7.7-update-2   active |
----

==== Packages Variables

You can also customize the packages to be installed and repos to be available on your nodes machines, you can find them under the packages section as follows:
[source,bash]
----
rhel_repos:                              # Repositories that will be available in the environment.
  - rhel-8-for-x86_64-baseos-rpms
  - rhel-8-for-x86_64-appstream-rpms

update_packages: false                   # Update all packages on system after configuration. true/false
----

Please note that available repos are linked to your RHN credentials that would have been provided on your secrets file.
[source,bash]
----
common_packages:                         # Packages to be installed on each node
  - unzip
  - bash-completion
  - wget
  - vim-enhanced
----