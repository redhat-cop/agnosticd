== Setting up ssh keys
Even if users have been created, no ssh keys have been defined, so we have not clue on how to log into the nodes machines, as we do not have a password.

We want to be able to create ssh keys for our users and we need to be able to let our users to access them, easily (in case someone wants to re use our config).

SSH keys configuration tasks are also listed on the `setup_lab_user.yml` file. Again, as we did with users, please add loops for all of the tasks (final result is shown below):

[source,bash]
----
[agilpipp-redhat.com@bastion configs]$ cd a-base-config/
[agilpipp-redhat.com@bastion a-base-config]$ vi setup_lab_user.yml
[...]
- name: create /home/{{ __ssh_dir }}/.ssh
  file:
    dest: /home/{{ __ssh_dir }}/.ssh
    mode: u+rwx,g-rwx,o-rwx
    owner: "{{ __ssh_dir }}"
    group: "users"
    state: directory
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __ssh_dir
  when: lab_users is defined

- name: copy the environment .pem key
  copy:
    src: "{{ output_dir }}/{{ env_authorized_key }}"
    dest: "/home/{{ __ssh_pem }}/.ssh/{{ env_authorized_key }}.pem"
    owner: "{{ __ssh_pem }}"
    group: "users"
    mode: u+r,g-rwx,o-rwx
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __ssh_pem
  when: lab_users is defined

- name: copy the environment .pub key
  copy:
    src: "{{ output_dir }}/{{ env_authorized_key }}.pub"
    dest: "/home/{{ __ssh_pub }}/.ssh/{{ env_authorized_key }}.pub"
    owner: "{{ __ssh_pub }}"
    group: "users"
    mode: u+r,g-rwx,o-rwx
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __ssh_pub
  when: lab_users is defined

- name: copy .ssh/config template
  template:
    src: ./files/ssh_config.j2
    dest: /home/{{ __ssh_conf }}/.ssh/config
    owner: "{{ __ssh_conf }}"
    group: "users"
    mode: u+r,g-rwx,o-rwx
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __ssh_conf
  when: lab_users is defined
----

As you can see, what we are doing here is:

1. Create .ssh directory for each user (task " `create /home/{{ __ssh_dir }}/.ssh` ")
2. Copy the PEM key file (task " `copy the environment .pem key` ")
3. Copy the PUB key file (task " `copy the environment .pub key` ")
4. Copy ssh config file template (task " `copy .ssh/config template` ")

At this point you may have already realized that we are not really creating keys here, but copying existing ones. 

The actual key is generated by the file `infra-role/infra-local-create-ssh-keys.yml` file called from `./agnosticd/ansible/configs/a-base-config/pre_software.yml` let's look at it:

[source,bash]
----
- name: Generate SSH keys
  shell: ssh-keygen -b 2048 -t rsa -f "{{output_dir}}/{{env_authorized_key}}" -q -N ""
  args:
    creates: "{{output_dir}}/{{env_authorized_key}}"
  when: set_env_authorized_key | bool

- name: fix permission
  file:
    path: "{{output_dir}}/{{env_authorized_key}}"
    mode: 0400
  when: set_env_authorized_key | bool

- name: Generate SSH pub key
  shell: ssh-keygen -y -f "{{output_dir}}/{{env_authorized_key}}" > "{{output_dir}}/{{env_authorized_key}}.pub"
  args:
    creates: "{{output_dir}}/{{env_authorized_key}}.pub"
  when: set_env_authorized_key | bool
----

There are a number of useful tasks related to the infrastructure already provided to us by Agnostic D via *roles*. Those can be found on `/agnosticd/ansible/roles-infra`, and are normally (but not only) called in the pre deploy stage, with the `pre_software.yml` file, as we just saw.

=== Setting the variables
Now that we have defined how we want ssh keys to be created, we do need to reference it in our variables file (`my_vars.yml` in this case).

The following variables are used in different roles, depending on what they are to accomplish:

* *set_env_authorized_key role*: This role creates the directories and copies the keys to the right directory for the user (root user).
* *bastion-lite*: This role creates the OpenSSH client-side configuration file. It configures the file stanzas for each node we define and stores them in the specified `output_dir`.

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ vi ~/my_vars.yml
# Environment Variables for SSH key generation and assignment
# # set_env_authorized_key role
set_env_authorized_key: true
env_authorized_key: "{{ guid }}key"

# # bastion-lite role
# Allow the use of an admin backdoor key for this machine
use_own_key: true
key_name: opentlc_admin_backdoor

# # files/hosts_template.j2 used by bastion
ansible_ssh_private_key_file: ~/.ssh/{{ key_name }}.pem
----

== Logging into our nodes

Now we need to log into our nodes and test everything is ok, that is, our users have been created and we are able to log into those machines.
First of all, we need to locate the ssh keys under our output directory:

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ ls /tmp/output_dir/
/tmp/output_dir/test-01key
/tmp/output_dir/test-01key.pub
/tmp/output_dir/a-base-config.test-01.osp_cloud_master_template.yaml
/tmp/output_dir/test-01_infra_ssh_key.pem
/tmp/output_dir/a-base-config_test-01_ssh_conf
/tmp/output_dir/a-base-config_test-01_ssh_known_hosts
----

We need to configure our ssh client if we want to be able to log in using the host name instead of the IP or the FQDN. Otherwise, we could simply use the IP of the new machines. In order to see the IPs, we simply need to run this Openstack command `openstack server list`.

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ openstack server list
+--------------------------------------+-----------+--------+-----------------------------------------------------+-------+---------+
| ID                                   | Name      | Status | Networks                                            | Image | Flavor  |
+--------------------------------------+-----------+--------+-----------------------------------------------------+-------+---------+
| aa2f36d3-9f16-4fa4-8387-ffb60091d0ee | control   | ACTIVE | test-01-node-network=192.168.47.26, 150.238.131.111 |       | 2c2g30d |
| 96892ba9-ed27-4f42-8d80-8fa7bdee796a | webserver | ACTIVE | test-01-node-network=192.168.47.18, 150.238.131.129 |       | 2c2g30d |
| 8938249c-9cc0-403f-9aaa-9ed93a29f2e2 | bastion   | ACTIVE | b1e1-testnet-network=192.168.0.36, 150.238.131.138  |       | 2c2g30d |
+--------------------------------------+-----------+--------+-----------------------------------------------------+-------+---------+

[agilpipp-redhat.com@bastion ansible]$ ssh cloud-user@150.238.131.111 -i /tmp/output_dir/test-01_infra_ssh_key.pem

Last login: Thu Dec 17 06:59:59 2020 from 150.238.131.138
[cloud-user@control ~]$ 
----

OpenSSH allows us to set up a per-user configuration file where you can store different SSH options for each remote machine you connect to. The client-side configuration file is named `config`, and it is stored in `.ssh` directory under each userâ€™s home directory (in our case, on the user we've log in our bastion with, that is, our RHPDS user). The `~/.ssh` directory is automatically created when the user runs the ssh command for the first time (that is, when we logged in our bastion machine).

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ ls .ssh/
authorized_keys  b1e1key.pem  b1e1key.pub  config  known_hosts
----

We have generated with the tasks explained before, this config file and stored it in our output dir. Let's take a look at it:

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ cat /tmp/output_dir/a-base-config_test-01_ssh_conf 
##### BEGIN ADDED BASTION PROXY HOST control a-base-config-test-01 ######
Host control 
  Hostname control.test-01.students.osp.opentlc.com
  IdentityFile /tmp/output_dir/test-01_infra_ssh_key.pem
  IdentitiesOnly yes
  User cloud-user
  ControlMaster auto
  ControlPath /tmp/test-01-%r-%h-%p
  ControlPersist 5m
  StrictHostKeyChecking no
  ConnectTimeout 60
  ConnectionAttempts 10
  UserKnownHostsFile /tmp/output_dir/a-base-config_test-01_ssh_known_hosts
##### END ADDED BASTION PROXY HOST control a-base-config-test-01 ######
##### BEGIN ADDED Node Proxy Config  webserver a-base-config-test-01 ######
Host webserver 150.238.131.129 
  Hostname 192.168.47.18
  User cloud-user
  IdentityFile /tmp/output_dir/test-01_infra_ssh_key.pem
  ProxyCommand ssh -F /tmp/output_dir/a-base-config_test-01_ssh_conf control -W %h:%p
  StrictHostKeyChecking no
  UserKnownHostsFile /tmp/output_dir/a-base-config_test-01_ssh_known_hosts
  ControlMaster auto
  ControlPath /tmp/test-01-%r-%h-%p
  ControlPersist 5m
##### END ADDED Node Proxy Config  webserver a-base-config-test-01 ######
----
The contents of the SSH client config file is organized into stanzas (sections). Each stanza starts with the Host directive and contains specific SSH options that are used when establishing a connection with the remote SSH server. So in our file, you can see two stanzas, one for control and one for webserver machines.

The Host directive can contain one pattern or a whitespace-separated list of patterns. The SSH client reads the configuration file stanza by stanza, and if more than one patterns match, the options from the first matching stanza take precedence. We are not using patterns in this particular file.

As you can see, Identity file is specified in the config file, so when we want to ssh into one of our servers, we won't need to use it.

Now we simply need to append this file to our existing config file, and test we can ssh as cloud-user in both servers without the need of specifying the identity file:

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ cat /tmp/output_dir/a-base-config_test-01_ssh_conf >> ~/.ssh/config
[agilpipp-redhat.com@bastion ~]$ ssh cloud-user@control
[cloud-user@control ~]$ logout
[agilpipp-redhat.com@bastion ~]$ ssh cloud-user@webserver
[cloud-user@webserver ~]$
[cloud-user@webserver ~]$ logout
----

But, what if I want to ssh with my newly created users? Currently, this is not possible, you have to ssh with `cloud-user` and then `sudo` into one of your users, and you may need to change that behaviour to fulfill the needs of your artifact.

In order to do so, we need to create a task, as we did for to create our users. We will use the same file, `setup_lab_user.yml` and we will add the following lines of code:

[source,bash]
----
- name: Add env authorized public key to users
  authorized_key:
    user: "{{ __ssh_auth }}"
    state: present
    key: "{{ lookup('file', '{{ output_dir }}/{{ env_authorized_key }}.pub') }}"
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __ssh_auth
  when: lab_users is defined
----

== Running the playbook
Again, now it is time to run the playbook, and this time, our users (amaya and wilson) will be able to log into the virtual machines we are deploying (do not forget to delete the stack before creating a new one, as you may exceed your quota).

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ cd agnosticd/ansible/
[agilpipp-redhat.com@bastion ansible]$ ansible-playbook main.yml -e @~/my_vars.yml -e @~/secrets.yml -e guid=test-01
----

Let's now test the SSH login:

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ ssh amaya@control -i /tmp/output_dir/test-01key
[amaya@control ~]$ 
----

== Clean up

In order to free resources, when you no longer need your deployment, or if you are making changes (i.e adding more nodes, users, changing the OS, etc.) it is highly recommended that you clean up your previous deployment as follows:

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ ansible-playbook destroy.yml \
   -e @~/my_vars.yml \ 
   -e @~/secrets.yaml \
   -e guid=amaya-test-01
----

Another way to clean your instances is via OpenStack, simply do `openstack stack delete <stack-name> or <stack-id>` as we have previously done in this lab.