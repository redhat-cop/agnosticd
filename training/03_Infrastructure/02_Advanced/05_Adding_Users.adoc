= How to add more than one user on your nodes machines

It is a very common practice to have multiple users on the nodes on your deployment, to cover different use cases of real life.

In this part of the training we will cover how to create new users on your machines and how the SSH keys are created and pushed into the machines.

In order to create new users, we are going to extend the possibilities of the `a-base-config` config to adapt it to our needs. This is a very common practice when developing with agnosticD.

== Adding users

=== Add tasks on tasks file post_software.yml

First step, is to locate the file in which users are created. This file is not on a-base-config, since this is a basic config that does not create extra users on the nodes. There are multiple configs that do create users, let's find out which one we could use:

[source,bash]
----
[agilpipp-redhat.com@bastion configs]$ find . -name setup_lab_user.yml
./ans-tower-lab-ng/setup_lab_user.yml
./ans-tower-lab/setup_lab_user.yml
./ansible-platform-foundations/setup_lab_user.yml
./ansible-tower-pntae/setup_lab_user.yml
./hacluster-breakfix1-pntae/setup_lab_user.yml
./hacluster-pntae/setup_lab_user.yml
./multi-network-app-pntae/setup_lab_user.yml
./pntae-jabn/setup_lab_user.yml
./pntae-vilt-bastion/setup_lab_user.yml
./smart-management-foundations/setup_lab_user.yml
./smart-management/setup_lab_user.yml
./three-tier-app/setup_lab_user.ym 
----

We can use, for instance, `./smart-management/setup_lab_user.yml` 
[source,bash]
----
[agilpipp-redhat.com@bastion configs]$ cat ./smart-management/setup_lab_user.yml
---
- name: Create lab-user
  user:
    name: "{{ lab_user }}"
    group: "users"
    state: present

- name: Enable sudoers
  lineinfile:
    path: '/etc/sudoers'
    state: present
    line: "{{ lab_user }}  ALL=(ALL) NOPASSWD: ALL"
----

This file could works for us, although it just creates one user, so we need to adapt it to our needs.

We copy the file into our config (our modified config, we could later simply rename it to make it *our config* and get that code approved and merged into the main agnosticD branch) and modify the tasks so it create several users.
[source,bash]
----
[agilpipp-redhat.com@bastion configs]$ cp ./smart-management/setup_lab_user.yml ./a-base-config/setup_lab_user.yml
[agilpipp-redhat.com@bastion configs]$ cd a-base-config/
[agilpipp-redhat.com@bastion a-base-config]$ vi setup_lab_user.yml
- name: Create lab-user
  user:
    name: "{{ __users }}"
    group: "users"
    state: present
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __users
  when: lab_users is defined

- name: Enable sudoers
  lineinfile:
    path: '/etc/sudoers'
    state: present
    line: "{{ __sudoers }}  ALL=(ALL) NOPASSWD: ALL"
    insertafter: 'EOF'
  loop: "{{ lab_users }}"
  loop_control:
    loop_var: __sudoers
  when: lab_users is defined
----

In order to be able to create multiple users, we use a loop, that iterates within `lab_users` and uses `__users` as control variable. We do same with `Enable sudoers tasks`, using `__sudoers` as the control variable.

*NOTE*: we could use `item` as a control variable, but declaring variables will allow us to use them later on, if needed.

At the end of the file, we also need to add `ignore_errors: true` at the `block` level, as we are not creating SSH keys just yet:
[source,bash]
----
- name: lab_user ssh setup
  block:
[...]
  ignore_errors: true
----

=== Defining user list

Now that we've been able to add more users, we'd need to define the list of users for our node machines. For that, simply define the `users` list in the vars file. In this case, we are going to create a new file that identifies the task we want to accomplish:

Copy the rhel8_server_on_osp.yml file and call it `2Users.yaml`.

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ cd agnosticd/ansible
[agilpipp-redhat.com@bastion ~]$ cp configs/a-base-config/sample_variables/rhel8_server_on_osp.yml \
  ~/2Users.yaml

[agilpipp-redhat.com@bastion configs]$ cd ~
[agilpipp-redhat.com@bastion ~]$ vi ~/2Users.yaml
[...]
lab_users:
  - amaya
  - wilson
[...]
----

=== Calling the right task
We've just added the task and users, but we have not called it yet. In order to make the task to be called, we need to place it in the right place, that is `post_software.yml`, as this is where checks, user notifications and clean up tasks take place.

[source,bash]
----
[agilpipp-redhat.com@bastion ]$ vi configs/a-base-config/post_software.yml
- name: Setup lab-user
  hosts: all
  become: true
  tasks:
    - name: Setup lab-user
      include_tasks: ./setup_lab_user.yml
----

NOTE: A fully configured file has been provided to you under `training/files/2Users.yaml`

Now, we simply need to execute the playbook 

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ cd agnosticd/ansible/
[agilpipp-redhat.com@bastion ansible]$ ansible-playbook main.yml \
   -e @~/2Users.yaml \
   -e @~/secrets.yaml \
   -e guid=amaya
----

When we run this playbook, we may see the following error:

[source,bash]
----
[...]
TASK [copy .ssh/config template] ***********************************************************************************************************************************************************
Wednesday 17 February 2021  05:52:32 -0500 (0:00:01.455)       0:03:44.196 ****
failed: [node] (item=amaya) => {"__ssh_conf": "amaya", "ansible_loop_var": "__ssh_conf", "changed": false, "msg": "Could not find or access './files/ssh_config.j2'\nSearched in:\n\t/home/agilpipp-redhat.com/agnosticd/ansible/configs/a-base-config/templates/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/configs/a-base-config/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/./configs/a-base-config/templates/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/./configs/a-base-config/./files/ssh_config.j2 on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option"}
failed: [node] (item=wilson) => {"__ssh_conf": "wilson", "ansible_loop_var": "__ssh_conf", "changed": false, "msg": "Could not find or access './files/ssh_config.j2'\nSearched in:\n\t/home/agilpipp-redhat.com/agnosticd/ansible/configs/a-base-config/templates/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/configs/a-base-config/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/./configs/a-base-config/templates/./files/ssh_config.j2\n\t/home/agilpipp-redhat.com/agnosticd/ansible/./configs/a-base-config/./files/ssh_config.j2 on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option"}
----

Analyzing the error, it complains it can not find a file, `./files/ssh_config.j2` in our config directory. Taking a look at it, we can see that is does not exist there:

[source,bash]
----
[agilpipp-redhat.com@bastion ~]$ cd agnosticd/ansible/configs/a-base-config/files/
[agilpipp-redhat.com@bastion ~/agnosticd/ansible/configs/a-base-config/files]$ ll
total 12
drwxr-xr-x. 2 agilpipp-redhat.com users  37 Feb 11 09:07 cloud_providers
-rw-r--r--. 1 agilpipp-redhat.com users 460 Feb 11 09:07 hosts_template.j2
-rw-r--r--. 1 agilpipp-redhat.com users 856 Feb 11 09:07 repos_template.j2
----

At this point, we need to either create a file that provides that functionality, or copy it, in case it already exists on any of the available configs in AgnosticD.

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible/configs]$ find . -name ssh_config.j2
./ans-tower-lab-ng/files/ssh_config.j2
./ans-tower-lab/files/ssh_config.j2
./ansible-gitOps-integration/files/ssh_config.j2
./ansible-multitier-infra/files/ssh_config.j2
./ansible-platform-foundations/files/ssh_config.j2
./ansible-tower-implementation/files/ssh_config.j2
./ansible-tower-pntae/files/ssh_config.j2
./ansible-windows-elt/files/ssh_config.j2
./ansible-workshops-rhel/files/ssh_config.j2
./hacluster-breakfix1-pntae/files/ssh_config.j2
./hacluster-pntae/files/ssh_config.j2
./multi-network-app-pntae/files/ssh_config.j2
./pntae-jabn/files/ssh_config.j2
./pntae-vilt-bastion/files/ssh_config.j2
./satellite-two-nodes/files/ssh_config.j2
./smart-management-foundations/files/ssh_config.j2
./smart-management/files/ssh_config.j2
./three-tier-app/files/ssh_config.j2

[agilpipp-redhat.com@bastion ~/agnosticd/ansible/configs]$ cp smart-management/files/ssh_config.j2 a-base-config/files
----

Now, we can re run our playbook again, but not before we clean the existing stack:

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ openstack stack delete base-stack-amaya
Are you sure you want to delete this stack(s) [y/N]? y

[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ansible-playbook main.yml -e @~/2Users.yaml -e @~/secrets.yaml
----

Now, it is time for us to test the users we have created, amaya and wilson.

First, let's see the IP for the machine we have just created:

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ openstack server list
+--------------------------------------+---------+--------+-----------------------------------------------------------------+-------+---------+
| ID                                   | Name    | Status | Networks                                                        | Image | Flavor  |
+--------------------------------------+---------+--------+-----------------------------------------------------------------+-------+---------+
| 515787a3-1058-4592-a63a-c024be6066a3 | node    | ACTIVE | amaya-node_network-network=192.168.47.202, 52.117.178.4         |       | 2c2g30d |
| 2e805693-7563-4cf0-ab29-a4ddff4d05df | bastion | ACTIVE | 7d77-testnet-network=192.168.0.46, 52.117.178.41                |       | 2c2g30d |
+--------------------------------------+---------+--------+-----------------------------------------------------------------+-------+---------+
----

And let's verify the keys are correctly created in the output directory previously specified in our variables file:

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ll /tmp/output_dir/amaya*
-rw-------. 1 agilpipp-redhat.com users 1675 Feb 17 06:23 /tmp/output_dir/amaya_infra_ssh_key.pem
-r--------. 1 agilpipp-redhat.com users 1675 Feb 17 05:48 /tmp/output_dir/amayakey
-rw-r--r--. 1 agilpipp-redhat.com users  409 Feb 17 05:48 /tmp/output_dir/amayakey.pub
----

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ssh -i /tmp/output_dir/amaya_infra_ssh_key.pem amaya@52.117.178.4
The authenticity of host '52.117.178.4 (52.117.178.4)' can't be established.
ECDSA key fingerprint is SHA256:QFPJWNqojPxy2esORPIbi6OJFrsT6/XrfrH8ySHa+2Y.
ECDSA key fingerprint is MD5:09:de:08:50:65:63:0e:d0:79:e4:72:f4:7c:f4:e0:af.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '52.117.178.4' (ECDSA) to the list of known hosts.
amaya@52.117.178.4's password:

[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$

[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ssh -i /tmp/output_dir/amaya_infra_ssh_key.pem wilson@52.117.178.4
wilson@52.117.178.4's password:

----

We can't log into the system with any of our newly created users, so, have them been created? As per Ansible's logs, it has, then, why can't we log with them? The answer is that we have created 2 extra users, but we have not created the ssh keys for them, for that, we'd also need to tweak the SSH keys playbooks.

Let's verify that our `cloud-user` (the one by default) and our student user can effectively log into the new system.

To log in with our student user, we need to use the username and password we have previously defined in our variables file, 2Users.yaml:

[source,bash]
----
student_name: "{{ guid }}-user"          
student_password: "r3dh4t1!" 
----

So we simply need to use those credentials:
[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ssh -i /tmp/output_dir/amaya_infra_ssh_key.pem amaya-user@52.117.178.4
amaya-user@52.117.178.4's password:


[amaya-user@node ~]$
----

Now, let's log with `cloud-user` and verify that users amaya and wilson have been, in fact, successfully created:

[source,bash]
----
[agilpipp-redhat.com@bastion ~/agnosticd/ansible]$ ssh -i /tmp/output_dir/amaya_infra_ssh_key.pem cloud-user@52.117.178.4


Last login: Wed Feb 17 06:26:37 2021 from 52.117.178.41
[cloud-user@node ~]$

[cloud-user@node ~]$ getent passwd amaya wilson
amaya:x:1002:100::/home/amaya:/bin/bash
wilson:x:1003:100::/home/wilson:/bin/bash
----

== Clean up

In order to free resources, when you no longer need your deployment, or if you are making changes (i.e adding more nodes, users, changing the OS, etc.) it is highly recommended that you clean up your previous deployment as follows:

[source,bash]
----
[agilpipp-redhat.com@bastion ansible]$ ansible-playbook destroy.yml \
   -e @~/my_vars.yml \ 
   -e @~/secrets.yaml
----

Another way to clean your instances is via OpenStack, simply do `openstack stack delete <stack-name> or <stack-id>` as we have previously done in this lab.