---
## TODO: What variables can we strip out of here to build complex variables?
## i.e. what can we add into group_vars as opposed to config_vars?
## Example: We don't really need "subdomain_base_short". If we want to use this,
## should just toss in group_vars/all.
### Also, we should probably just create a variable reference in the README.md
### For now, just tagging comments in line with configuration file.

### Vars that can be removed:
# use_satellite: true
# use_subscription_manager: false
# use_own_repos: false

###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

### Common Host settings

# Passwords are generate in Playbook
#tower_admin_password: 'changeme'
#automationcontroller_admin_password: '{{ tower_admin_password }}'

# tower_version: "4.0.0"  # not used by aap_deploy (yet)

# Version of the VSCode server as available at
# https://github.com/coder/code-server/releases
# vscode_server_version: 4.5.2  # use default version from role vscode-server
vscode_server_nginx_https_port: 8443

# Gitea variables
gitea_version: '1.21.9'
gitea_require_signin: false  # we shouldn't have anything secret in the Git repos
gitea_vm_repos:  # list of Git repos to migrate/copy into Gitea
  - name: playbooks-adv-controller
    url: https://gitlab.com/ansible-labs-crew/playbooks-adv-controller.git
  - name: shared-apache-role
    url: https://gitlab.com/ansible-labs-crew/shared-apache-role.git
  - name: playbooks-dev
    url: https://gitlab.com/ansible-labs-crew/playbooks-dev.git
  - name: playbooks-ops
    url: https://gitlab.com/ansible-labs-crew/playbooks-ops.git
gitea_vm_https_port: 488

## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another "just like it"
guid: defaultguid

install_bastion: true
install_common: true
install_ipa_client: false
# Create student user, most likely "lab-user" with SSH password login
install_student_user: true
bastion_student_user_ansible: true  # student user can use Ansible
#software_to_deploy: tower  # replaced by role aap_deploy in software

## Set Tower manifest and installer tarball location and creds if not specified in agnosticv vars file
#tower_license_manifest:
#  type: url
#  url: "changme"
#  username: "changeme"
#  password: "changeme"

#automationcontroller_installer:
#  url: "changeme"
#  username: "changeme"
#  password: "changeme"

### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
env_authorized_key: "{{guid}}key"
set_env_authorized_key: true

### AWS EC2 Environment settings

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will create your Public DNS entries
# This only needs to be defined if your CF template uses route53
HostedZoneId: Z3IHLWJZOU9SRT
# The region to be used, if not specified by -e in the command line
aws_region: ap-southeast-2
# The key that is used to

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

## Environment Sizing
bastion_instance_type: "t3a.medium"
bastion_instance_image: "RHEL93GOLD-latest"

# Set tower_instance_count to "1" for single-instance Tower (no cluster), "3" for Tower cluster (Set pgdb to "1" then!)
tower_instance_name: autoctl  # default is tower
tower_instance_count: 3
tower_instance_type: "t3a.large"
tower_instance_image: "RHEL93GOLD-latest"

# Set to "0" if no cluster, "1" for Tower cluster
pgdb_instance_name: pgdb  # default is pgdb
pgdb_instance_count: 1
# uses tower_instance type and image

node_instance_count: 3
node_instance_type: "t3a.medium"
node_instance_image: "RHEL93GOLD-latest"

worker_instance_count: 0
worker_instance_type: "t3a.medium"
worker_instance_image: "RHEL93GOLD-latest"

support_instance_count: 0
support_instance_type: "t3a.medium"
support_instance_image: "RHEL93GOLD-latest"

# Set to 1 to deploy Private Automation Hub
pah_instance_count: 0
pah_instance_type: "t3a.large"
pah_instance_image: "RHEL93GOLD-latest"

exec_instance_count: 0  # increase also tower_expected_instances accordingly!
exec_instance_type: "t3a.medium"
exec_instance_image: "RHEL93GOLD-latest"

subnets:
  - name: PublicSubnet
    cidr: "192.168.1.0/24"
    routing_table: true
  - name: PrivateSubnet
    cidr: "192.168.2.0/24"
    routing_table: false

security_groups:
  - name: BastionSG
    rules:
      - name: BasSSHPublic
        description: "SSH public"
        from_port: 22
        to_port: 22
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: BasHTTPPublic
        description: "HTTP public"
        from_port: 80
        to_port: 80
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: BasHTTPSPublic
        description: "HTTPS public"
        from_port: 443
        to_port: 443
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: BasHTTPS2Public
        description: "HTTPS public for VSCode"
        from_port: "{{ vscode_server_nginx_https_port }}"
        to_port: "{{ vscode_server_nginx_https_port }}"
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: BasHTTPS3Public
        description: "HTTPS public for Gitea"
        from_port: "{{ gitea_vm_https_port }}"
        to_port: "{{ gitea_vm_https_port }}"
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress

  - name: TowerSG
    rules:
      - name: SSHTower
        description: "SSH public"
        from_port: 22
        to_port: 22
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: HTTPTower
        description: "HTTP public"
        from_port: 80
        to_port: 80
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: HTTPSTower
        description: "HTTP public"
        from_port: 443
        to_port: 443
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: Mesh
        description: "Mesh"
        from_port: 27199
        to_port: 27199
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: BasTowerTcp
        description: "ALL from bastion tcp"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: BastionSG
        rule_type: Ingress
      - name: BasTowerUdp
        description: "ALL from bastion udp"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: BastionSG
        rule_type: Ingress
      - name: AllInternaltcp
        description: "All other nodes tcp"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: HostSG
        rule_type: Ingress
      - name: AllInternaludp
        description: "All other nodes udp"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: HostSG
        rule_type: Ingress
      - name: AllTowerNodestcp
        description: "All tower nodes tcp"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: TowerSG
        rule_type: Ingress
      - name: AllTowerNodesudp
        description: "All tower nodes udp"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: TowerSG
        rule_type: Ingress
  - name: HostSG
    rules:
      - name: Postgresql
        description: "PostgreSql"
        from_port: 5432
        to_port: 5432
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: Mesh
        description: "Mesh"
        from_port: 27199
        to_port: 27199
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
      - name: HostUDPPorts
        description: "Only from Itself udp"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: HostSG
        rule_type: Ingress
      - name: HostTCPPorts
        description: "Only from Itself tcp"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: HostSG
        rule_type: Ingress
      - name: TowerUDPPorts
        description: "Only from tower"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: TowerSG
        rule_type: Ingress
      - name: TowerTCPPorts
        description: "Only from tower"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: TowerSG
        rule_type: Ingress
      - name: BastionUDPPorts
        description: "Only from bastion"
        from_port: 0
        to_port: 65535
        protocol: udp
        from_group: BastionSG
        rule_type: Ingress
      - name: BastionTCPPorts
        description: "Only from bastion"
        from_port: 0
        to_port: 65535
        protocol: tcp
        from_group: BastionSG
        rule_type: Ingress

instances:
  - name: "bastion"
    count: 1
    unique: true
    public_dns: true
    dns_loadbalancer: true
    security_groups:
      - BastionSG
    image: "{{ bastion_instance_image }}"
    flavor:
      ec2: "{{bastion_instance_type}}"
      azure: "{{bastion_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{root_filesystem_size}}"
    subnet: PublicSubnet

  - name: "{{ tower_instance_name | default('tower') }}"
    count: "{{ tower_instance_count }}"
    security_groups:
      - TowerSG
    public_dns: true
    dns_loadbalancer: true
    image: "{{ tower_instance_image }}"
    flavor:
      "ec2": "{{tower_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "automationcontroller"
      - key: "ostype"
        value: "linux"
    subnet: PublicSubnet

  - name: "{{ pgdb_instance_name | default('pgdb') }}"
    count: "{{pgdb_instance_count}}"
    unique: true
    public_dns: false
    security_groups:
      - TowerSG
      - HostSG
    image: "{{ tower_instance_image }}"
    flavor:
      "ec2": "{{tower_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "automationcontroller_database"
      - key: "ostype"
        value: "rhel"
    subnet: PublicSubnet

  - name: "node"
    count: "{{node_instance_count}}"
    public_dns: false
    security_groups:
      - TowerSG
      - HostSG
    image: "{{ node_instance_image }}"
    flavor:
      "ec2": "{{node_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "managed-nodes"
      - key: "ostype"
        value: "rhel"
    subnet: PublicSubnet

  - name: "pah"
    count: "{{pah_instance_count}}"
    public_dns: true
    unique: true
    security_groups:
      - TowerSG
      - HostSG
    image: "{{ pah_instance_image }}"
    flavor:
      "ec2": "{{pah_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "pah"
      - key: "ostype"
        value: "rhel"
    subnet: PublicSubnet

  - name: "support"
    count: "{{support_instance_count}}"
    public_dns: false
    security_groups:
      - TowerSG
      - HostSG
    image: "{{ support_instance_image }}"
    flavor:
      "ec2": "{{support_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "support"
      - key: "ostype"
        value: "rhel"
    subnet: PublicSubnet

  # execution hosts
  - name: "exec"  # name must be strictly alphanumeric!
    count: "{{exec_instance_count}}"
    public_dns: false
    security_groups:
      - TowerSG
      - HostSG
    image: "{{ exec_instance_image }}"
    flavor:
      "ec2": "{{exec_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "execution_nodes"
      - key: "ostype"
        value: "rhel"
    subnet: PublicSubnet  #PrivateSubnet


######### Worker instances #########
instances_worker:
  - name: "worker"
    count: "{{worker_instance_count}}"
    security_groups:
      - HostSG
      - TowerSG
    public_dns: false
    dns_loadbalancer: false
    image: "{{ worker_instance_image }}"
    flavor:
      "ec2": "{{worker_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "workers"
      - key: "ostype"
        value: "linux"
    subnet: PublicSubnet  #PrivateSubnet
#########

  # - name: "worker{{target_regions[1].name}}"
  #   count: "{{worker_instance_count}}"
  #   security_groups:
  #     - HostSG
  #     - TowerSG
  #   public_dns: false
  #   dns_loadbalancer: false
  #   image: "{{ worker_instance_image }}"
  #   flavor:
  #     "ec2": "{{worker_instance_type}}"
  #   tags:
  #     - key: "AnsibleGroup"
  #       value: "workers"
  #     - key: "ostype"
  #       value: "linux"
  #     - key: Worker_region
  #       value: "{{ target_regions[1].name }}"
  #   subnet: PublicSubnet
  #   #########

  # - name: "worker{{target_regions[2].name}}"
  #   count: "{{worker_instance_count}}"
  #   security_groups:
  #     - HostSG
  #     - TowerSG
  #   public_dns: false
  #   dns_loadbalancer: false
  #   image: "{{ worker_instance_image }}"
  #   flavor:
  #     "ec2": "{{worker_instance_type}}"
  #   tags:
  #     - key: "AnsibleGroup"
  #       value: "workers"
  #     - key: "ostype"
  #       value: "linux"
  #     - key: Worker_region
  #       value: "{{ target_regions[2].name }}"
  #   subnet: PublicSubnet
    #######*************#############

###### VARIABLES YOU SHOULD ***NOT*** CONFIGURE FOR YOUR DEPLOYEMNT
###### You can, but you usually wouldn't need to.
ansible_user: ec2-user
remote_user: ec2-user

common_packages:
  - python
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - git
  - vim-enhanced
  - at
  - python-pip
  - gcc
  - ansible

common_packages_el8:
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - git
  - vim-enhanced
  - at
  - python3-pip
  - gcc
  - ansible

common_packages_el9:
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - nano
  - git
  - vim-enhanced
  - at
  - python3-pip
  - gcc
  - ansible

# -------------------------------------------------------------------
# Repos
# -------------------------------------------------------------------
use_content_view: false  # if true, _all_ available repos are activated
rhel_repos:
  - rhel-9-for-x86_64-appstream-rpms
  - rhel-9-for-x86_64-baseos-rpms
  - ansible-automation-platform-2.4-for-rhel-9-x86_64-rpms

#If using repo_method: satellite, you must set these values as well.
# satellite_url: satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"
# Those two following options are overwritten by common.yml!
repo_method: satellite  # Other options are: file, satellite and rhn
update_packages: true

project_tag: "{{ env_type }}-{{ guid }}"

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"

tower_public_dns: "towerlb.{{subdomain_base}}."

#tower_public_dns: "tower.{{subdomain_base}}."
bastion_public_dns: "bastion.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
# we don't use this anymore <sborenst>
# activedirectory_public_dns: "ad.{{subdomain_base}}."
# activedirectory_public_dns_chomped: "ad.{{subdomain_base}}"
#
# vpcid_cidr_block: "192.168.0.0/16"
# vpcid_name_tag: "{{subdomain_base}}"
#
# az_1_name: "{{ aws_region }}a"
# az_2_name: "{{ aws_region }}b"
#
# subnet_private_1_cidr_block: "192.168.2.0/24"
# subnet_private_1_az: "{{ az_2_name }}"
# subnet_private_1_name_tag: "{{subdomain_base}}-private"
#
# subnet_private_2_cidr_block: "192.168.1.0/24"
# subnet_private_2_az: "{{ az_1_name }}"
# subnet_private_2_name_tag: "{{subdomain_base}}-private"
#
# subnet_public_1_cidr_block: "192.168.10.0/24"
# subnet_public_1_az: "{{ az_1_name }}"
# subnet_public_1_name_tag: "{{subdomain_base}}-public"
#
# subnet_public_2_cidr_block: "192.168.20.0/24"
# subnet_public_2_az: "{{ az_2_name }}"
# subnet_public_2_name_tag: "{{subdomain_base}}-public"
#
# dopt_domain_name: "{{ aws_region }}.compute.internal"
#
# rtb_public_name_tag: "{{subdomain_base}}-public"
# rtb_private_name_tag: "{{subdomain_base}}-private"
#
#
# cf_template_description: "{{ env_type }}-{{ guid }} Ansible Agnostic Deployer"
tower_run: false

#infra_workloads:
#  - tower-copy-ssh
#  - tower-license-injector
#  - cleanup-tower-default
#  - tower-settings-update
#  - tower-pip-packages
#  - tower-user-create
#  - tower-custom-venv
#  - tower-org-create
#  - tower-credential-create
#  - tower-project-create
#  - tower-inventory-create
#  - tower-job-template-create
#  - tower-babylon-job-runner

### Variables to check against in the tower_validate role, they must be aligned with what you expect
# Set to "1" if single-instance Tower, "3" for Tower cluster
# Execution hosts also need to be counted
tower_expected_instances: 3           # number of instances in the cluster
tower_expected_instance_groups: 2     # number of instance groups in the cluster
# AAP 2.2+ doesn't care too much about subscription count, hence 1 managed node
tower_expected_licensed_min_nodes: 1  # how many managed nodes must be covered
tower_expected_licensed_min_days: 2   # how many days there must be left on the license
