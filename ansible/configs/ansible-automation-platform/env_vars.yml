---
## TODO: What variables can we strip out of here to build complex variables?
## i.e. what can we add into group_vars as opposed to config_vars?
## Example: We don't really need "subdomain_base_short". If we want to use this,
## should just toss in group_vars/all.
### Also, we should probably just create a variable reference in the README.md
### For now, just tagging comments in line with configuration file.

### Vars that can be removed:
# use_satellite: true
# use_subscription_manager: false
# use_own_repos: false

###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

### Common Host settings

repo_method: satellite # Other Options are: file, satellite and rhn

# Passwords are generate in Playbook
#tower_admin_password: 'changeme'
#automationcontroller_admin_password: '{{ tower_admin_password }}'

# tower_version: "4.0.0"  # not used by deploy_automationcontroller (yet)
# Do you want to run a full yum update
update_packages: false
#If using repo_method: satellite, you must set these values as well.
# satellite_url: satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"

## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another "just like it"
guid: defaultguid

install_bastion: true
install_common: true
install_ipa_client: false
# Create student user, most likely "lab-user" with SSH password login
install_student_user: true
#software_to_deploy: tower  # replaced by role deploy_automationcontroller in software

## Set Tower manifest and installer tarball location and creds if not specified in agnosticv vars file
#tower_license_manifest:
#  type: url
#  url: "changme"
#  username: "changeme"
#  password: "changeme"

#automationcontroller_installer:
#  url: "changeme"
#  username: "changeme"
#  password: "changeme"

### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
# if you set "use_own_key" to false your PRIVATE key will be copied to the bastion. (This is {{key_name}})
use_own_key: true
env_authorized_key: "{{guid}}key"
ansible_ssh_private_key_file: ~/.ssh/{{key_name}}.pem
set_env_authorized_key: true

### AWS EC2 Environment settings

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will create your Public DNS entries
# This only needs to be defined if your CF template uses route53
HostedZoneId: Z3IHLWJZOU9SRT
# The region to be used, if not specified by -e in the command line
aws_region: ap-southeast-2
# The key that is used to
key_name: "default_key_name"

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

## Environment Sizing
bastion_instance_type: "t2.medium"
bastion_instance_image: "RHEL84GOLD"

# Set tower_instance_count to "1" for single-instance Tower (no cluster), "3" for Tower cluster (Set pgdb to "1" then!)
tower_instance_name: autoctl  # default is tower
tower_instance_count: 3
tower_instance_type: "t2.large"
tower_instance_image: "RHEL84GOLD"

# Set to "0" if no cluster, "1" for Tower cluster
pgdb_instance_name: pgdb  # default is pgdb
pgdb_instance_count: 1

node_instance_count: 3
node_instance_type: "t2.large"
node_instance_image: "RHEL84GOLD"

worker_instance_count: 0
worker_instance_type: "t2.large"
worker_instance_image: "RHEL84GOLD"

support_instance_count: 0
support_instance_type: "t2.large"
support_instance_image: "RHEL84GOLD"
isosupport_instance_count: 0

subnets:
- name: PublicSubnet
  cidr: "192.168.1.0/24"
  routing_table: true
- name: PrivateSubnet
  cidr: "192.168.2.0/24"
  routing_table: false

security_groups:
- name: BastionSG
  rules:
  - name: BasSSHPublic
    description: "SSH public"
    from_port: 22
    to_port: 22
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: BasHTTPPublic
    description: "HTTP public"
    from_port: 80
    to_port: 80
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: BasHTTPSPublic
    description: "HTTPS public"
    from_port: 443
    to_port: 443
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
- name: TowerSG
  rules:
  - name: SSHTower
    description: "SSH public"
    from_port: 22
    to_port: 22
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: HTTPTower
    description: "HTTP public"
    from_port: 80
    to_port: 80
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: HTTPSTower
    description: "HTTP public"
    from_port: 443
    to_port: 443
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: RabbitMq
    description: "RabbitMq"
    from_port: 5672
    to_port: 5672
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: EPMD
    description: "EPMD"
    from_port: 4369
    to_port: 4369
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: RabbitMqCLi
    description: "RabbitMq Cli"
    from_port: 25672
    to_port: 25672
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: RabbitMqAPi
    description: "RabbitMq Api"
    from_port: 15672
    to_port: 15672
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: RabbitMqCliTools
    description: "RabbitMq CLi tools"
    from_port: 35672
    to_port: 35672
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: BasTowerTcp
    description: "ALL from bastion tcp"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: BastionSG
    rule_type: Ingress
  - name: BasTowerUdp
    description: "ALL from bastion udp"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: BastionSG
    rule_type: Ingress
  - name: AllInternaltcp
    description: "All other nodes tcp"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: HostSG
    rule_type: Ingress
  - name: AllInternaludp
    description: "All other nodes udp"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: HostSG
    rule_type: Ingress
  - name: AllTowerNodestcp
    description: "All tower nodes tcp"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: TowerSG
    rule_type: Ingress
  - name: AllTowerNodesudp
    description: "All tower nodes udp"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: TowerSG
    rule_type: Ingress
- name: HostSG
  rules:
  - name: HostUDPPorts
    description: "Only from Itself udp"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: HostSG
    rule_type: Ingress
  - name: Postgresql
    description: "PostgreSql"
    from_port: 5432
    to_port: 5432
    protocol: tcp
    cidr: "0.0.0.0/0"
    rule_type: Ingress
  - name: HostTCPPorts
    description: "Only from Itself tcp"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: HostSG
    rule_type: Ingress
  - name: TowerUDPPorts
    description: "Only from tower"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: TowerSG
    rule_type: Ingress
  - name: TowerTCPPorts
    description: "Only from tower"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: TowerSG
    rule_type: Ingress
  - name: BastionUDPPorts
    description: "Only from bastion"
    from_port: 0
    to_port: 65535
    protocol: udp
    group: BastionSG
    rule_type: Ingress
  - name: BastionTCPPorts
    description: "Only from bastion"
    from_port: 0
    to_port: 65535
    protocol: tcp
    group: BastionSG
    rule_type: Ingress

instances:
- name: "bastion"
  count: 1
  unique: true
  public_dns: true
  dns_loadbalancer: true
  security_groups:
  - BastionSG
  image: "{{ bastion_instance_image }}"
  flavor:
    ec2: "{{bastion_instance_type}}"
    azure: "{{bastion_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "bastions"
  - key: "ostype"
    value: "linux"
  rootfs_size: "{{root_filesystem_size}}"
  subnet: PublicSubnet

- name: "{{ tower_instance_name | default('tower') }}"
  count: "{{ tower_instance_count }}"
  security_groups:
  - TowerSG
  public_dns: true
  dns_loadbalancer: true
  image: "{{ tower_instance_image }}"
  flavor:
    "ec2": "{{tower_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "automationcontroller"
  - key: "ostype"
    value: "linux"
  subnet: PublicSubnet

- name: "{{ pgdb_instance_name | default('pgdb') }}"
  count: "{{pgdb_instance_count}}"
  unique: true
  public_dns: false
  security_groups:
  - TowerSG
  - HostSG
  image: "{{ tower_instance_image }}"
  flavor:
    "ec2": "{{tower_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "automationcontroller_database"
  - key: "ostype"
    value: "rhel"
  key_name: "{{key_name}}"
  subnet: PublicSubnet

- name: "node"
  count: "{{node_instance_count}}"
  public_dns: false
  security_groups:
  - TowerSG
  - HostSG
  image: "{{ node_instance_image }}"
  flavor:
    "ec2": "{{node_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "managed-nodes"
  - key: "ostype"
    value: "rhel"
  key_name: "{{key_name}}"
  subnet: PublicSubnet

- name: "support"
  count: "{{support_instance_count}}"
  public_dns: false
  security_groups:
  - TowerSG
  - HostSG
  image: "{{ support_instance_image }}"
  flavor:
    "ec2": "{{support_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "support"
  - key: "ostype"
    value: "rhel"
  key_name: "{{key_name}}"
  subnet: PublicSubnet

# isolated support hosts
- name: "isosupport"  # name must be strictly alphanumeric!
  count: "{{isosupport_instance_count}}"
  public_dns: false
  security_groups:
  - TowerSG
  - HostSG
  image: "{{ support_instance_image }}"
  flavor:
    "ec2": "{{support_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "support"
  - key: "ostype"
    value: "rhel"
  key_name: "{{key_name}}"
  subnet: PublicSubnet  #PrivateSubnet


######### Worker instances #########
instances_worker:
- name: "worker"
  count: "{{worker_instance_count}}"
  security_groups:
  - HostSG
  - TowerSG
  public_dns: false
  dns_loadbalancer: false
  image: "{{ worker_instance_image }}"
  flavor:
    "ec2": "{{worker_instance_type}}"
  tags:
  - key: "AnsibleGroup"
    value: "workers"
  - key: "ostype"
    value: "linux"
  subnet: PublicSubnet  #PrivateSubnet
#########

  # - name: "worker{{target_regions[1].name}}"
  #   count: "{{worker_instance_count}}"
  #   security_groups:
  #     - HostSG
  #     - TowerSG
  #   public_dns: false
  #   dns_loadbalancer: false
  #   image: "{{ worker_instance_image }}"
  #   flavor:
  #     "ec2": "{{worker_instance_type}}"
  #   tags:
  #     - key: "AnsibleGroup"
  #       value: "workers"
  #     - key: "ostype"
  #       value: "linux"
  #     - key: Worker_region
  #       value: "{{ target_regions[1].name }}"
  #   subnet: PublicSubnet
  #   #########

  # - name: "worker{{target_regions[2].name}}"
  #   count: "{{worker_instance_count}}"
  #   security_groups:
  #     - HostSG
  #     - TowerSG
  #   public_dns: false
  #   dns_loadbalancer: false
  #   image: "{{ worker_instance_image }}"
  #   flavor:
  #     "ec2": "{{worker_instance_type}}"
  #   tags:
  #     - key: "AnsibleGroup"
  #       value: "workers"
  #     - key: "ostype"
  #       value: "linux"
  #     - key: Worker_region
  #       value: "{{ target_regions[2].name }}"
  #   subnet: PublicSubnet
    #######*************#############

###### VARIABLES YOU SHOULD ***NOT*** CONFIGURE FOR YOUR DEPLOYEMNT
###### You can, but you usually wouldn't need to.
ansible_user: ec2-user
remote_user: ec2-user

common_packages:
- python
- unzip
- bash-completion
- tmux
- bind-utils
- wget
- git
- vim-enhanced
- at
- python-pip
- gcc
- ansible

common_packages_el8:
- unzip
- bash-completion
- tmux
- bind-utils
- wget
- git
- vim-enhanced
- at
- python3-pip
- gcc
- ansible

rhel_repos:
- automation-hub-4.2-for-rhel-8-x86_64-rpms
- codeready-builder-for-rhel-8-x86_64-rpms
- rhel-8-for-x86_64-appstream-rpms
- rhel-8-for-x86_64-baseos-rpms

project_tag: "{{ env_type }}-{{ guid }}"

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"

tower_public_dns: "towerlb.{{subdomain_base}}."

#tower_public_dns: "tower.{{subdomain_base}}."
bastion_public_dns: "bastion.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
# we don't use this anymore <sborenst>
# activedirectory_public_dns: "ad.{{subdomain_base}}."
# activedirectory_public_dns_chomped: "ad.{{subdomain_base}}"
#
# vpcid_cidr_block: "192.168.0.0/16"
# vpcid_name_tag: "{{subdomain_base}}"
#
# az_1_name: "{{ aws_region }}a"
# az_2_name: "{{ aws_region }}b"
#
# subnet_private_1_cidr_block: "192.168.2.0/24"
# subnet_private_1_az: "{{ az_2_name }}"
# subnet_private_1_name_tag: "{{subdomain_base}}-private"
#
# subnet_private_2_cidr_block: "192.168.1.0/24"
# subnet_private_2_az: "{{ az_1_name }}"
# subnet_private_2_name_tag: "{{subdomain_base}}-private"
#
# subnet_public_1_cidr_block: "192.168.10.0/24"
# subnet_public_1_az: "{{ az_1_name }}"
# subnet_public_1_name_tag: "{{subdomain_base}}-public"
#
# subnet_public_2_cidr_block: "192.168.20.0/24"
# subnet_public_2_az: "{{ az_2_name }}"
# subnet_public_2_name_tag: "{{subdomain_base}}-public"
#
# dopt_domain_name: "{{ aws_region }}.compute.internal"
#
# rtb_public_name_tag: "{{subdomain_base}}-public"
# rtb_private_name_tag: "{{subdomain_base}}-private"
#
#
# cf_template_description: "{{ env_type }}-{{ guid }} Ansible Agnostic Deployer"
tower_run: false

#infra_workloads:
#  - tower-copy-ssh
#  - tower-license-injector
#  - cleanup-tower-default
#  - tower-settings-update
#  - tower-pip-packages
#  - tower-user-create
#  - tower-custom-venv
#  - tower-org-create
#  - tower-credential-create
#  - tower-project-create
#  - tower-inventory-create
#  - tower-job-template-create
#  - tower-babylon-job-runner

### Variables to check against in the tower_validate role, they must be aligned with what you expect
# Set to "1" if single-instance Tower, "3" for Tower cluster
tower_expected_instances: 3            # number of instances in the cluster
tower_expected_instance_groups: 1      # number of instance groups in the cluster
tower_expected_licensed_min_nodes: 20  # how many managed nodes must be covered
tower_expected_licensed_min_days: 2    # how many days there must be left on the license
