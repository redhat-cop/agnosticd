---
- import_playbook: ../../setup_runtime.yml

- name: Teardown OpenStack project and resources
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  tasks:
  - name: Cleanup OpenStack DNS and IPs
    when: cloud_provider == 'osp'
    block:
    - name: Run infra-osp-dns to cleanup DNS
      include_role:
        name: infra-osp-dns
      vars:
        _dns_state: absent

    - name: Remove DNS entry for OpenShift API and ingress
      nsupdate:
        server: "{{ osp_cluster_dns_server }}"
        zone: "{{ osp_cluster_dns_zone }}"
        record: "{{ item }}.{{ guid }}"
        type: A
        key_name: "{{ ddns_key_name }}"
        key_algorithm: "{{ ddns_key_algorithm | d('hmac-md5') }}"
        key_secret: "{{ ddns_key_secret }}"
        state: absent
      loop:
        - "api"
        - "*.apps"
      when:
        - openshift_fip_provision
        - use_dynamic_dns

    - name: Run infra-osp-resources-destroy role
      include_role:
        name: infra-osp-resources-destroy

- name: AWS Teardown Backup event log of the user
  hosts: localhost
  connection: local
  gather_facts: False
  become: no
  tasks:
  - name: Tear down AWS
    when: cloud_provider == 'ec2'
    environment:
      AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
      AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
      AWS_DEFAULT_REGION: "{{aws_region_final|d(aws_region)}}"
    block:
    - name: Create Infra Key
      include_role:
        name: infra-ec2-ssh-key
      when: install_infra_ssh_key | default(false) | bool

    - name: Get fact for stack
      cloudformation_facts:
        stack_name: "{{ project_tag }}"
      register: stack_facts

    - when: project_tag in stack_facts.ansible_facts.cloudformation
      block:
      - name: Grab and set stack creation_time
        set_fact:
          stack_creation_time: >-
            {{ stack_facts.ansible_facts.cloudformation[project_tag].stack_description.creation_time }}
          stack_status: >-
            {{ stack_facts.ansible_facts.cloudformation[project_tag].stack_description.stack_status }}

- name: Building the Inventory for different clouds
  hosts: localhost
  connection: local
  become: false
  tasks:
  - name: Get SSH public key
    set_fact:
      ssh_key: "~/.ssh/{{key_name}}.pem"
      ssh_key_data: "{{lookup('file', '~/.ssh/{{key_name}}.pub')}}"
    tags:
    - set_existing_ssh_key
    - must
    - create_inventory
    when:
    - not set_env_authorized_key | bool
    - cloud_provider != 'osp'
  - name: Get SSH public key
    set_fact:
      ssh_key: "{{output_dir}}/{{env_authorized_key}}"
      ssh_key_data: "{{lookup('file', '{{output_dir}}/{{env_authorized_key}}.pub')}}"
    tags:
    - set_generated_ssh_key
    - must
    - create_inventory
    when:
    - set_env_authorized_key | bool
    - cloud_provider != 'osp'

  - name: Run infra-ec2-create-inventory Role
    when: cloud_provider == 'ec2'
    include_role:
      name: infra-ec2-create-inventory

  - name: Run infra-azure-create-inventory Role
    when: cloud_provider == 'azure'
    include_role:
      name: infra-azure-create-inventory

  - name: Run infra-gcp-create-inventory Role
    when: cloud_provider == 'gcp'
    include_role:
      name: infra-gcp-create-inventory

  - name: Cloud Specific SSH Config Setup
    when:
    - cloud_provider != 'osp'
    - groups["bastions"] is defined
    - groups["bastions"] | length > 0
    include_role:
      name: infra-common-ssh-config-generate

  # WK: I don't think this works at all. Can't import a playbook if it's not top level...
  # - name: Cloud Specific SSH Config Setup
  #   when:
  #   - cloud_provider != 'osp'
  #   - groups["bastions"] is defined
  #   - groups["bastions"] | length > 0
  #   import_playbook: ../../cloud_providers/{{ cloud_provider }}_ssh_config_setup.yml

- name: Set ssh extra args for all hosts, use ssh_config just created
  hosts: all
  gather_facts: false
  any_errors_fatal: true
  ignore_errors: false
  tasks:
  - name: add -F option ansible_ssh_extra_args
    set_fact:
      ansible_ssh_extra_args: "{{ ansible_ssh_extra_args|d() }} -F {{ hostvars['localhost'].ansible_ssh_config }}"

- name: Having the OpenShift installer cleanup what it did
  hosts: bastions
  gather_facts: false
  become: false
  tasks:
  - name: OpenShift uninstall for all platforms except OpenStack
    when: cloud_provider != 'osp'
    block:
    - name: Test the bastion host is available, if not skip host-ocp4-destroy
      wait_for_connection:
        timeout: 60
      register: bwait
      ignore_errors: true
    - name: AWS Credentials to Bastion for OpenShift Installer to use
      when: cloud_provider == 'ec2'
      block:
      - name: Add aws credentials for sandboxes
        when: 
        - hostvars.localhost.student_access_key_id | d("") | length > 0
        - hostvars.localhost.student_access_secret_key | d("") | length > 0
        become: no
        blockinfile:
          path: "~/.aws/credentials"
          create: true
          insertbefore: BOF
          marker: "# {mark} ANSIBLE MANAGED BLOCK Student Credentials"
          block: |-
            [default]
            aws_access_key_id = {{ hostvars.localhost.student_access_key_id }}
            aws_secret_access_key = {{ hostvars.localhost.student_secret_access_key }}

      # Only use shared account when no student account has been configured
      - name: Add aws credentials for shared account
        when:
        - hostvars.localhost.student_access_key_id | d("") | length == 0
        - aws_access_key_id | d("") | length > 0
        - aws_secret_access_key | d("") | length > 0
        become: no
        blockinfile:
          state: present
          path: "~/.aws/credentials"
          create: yes
          insertbefore: BOF
          marker: "# {mark} ANSIBLE MANAGED BLOCK Default Credentials"
          block: |-
            [default]
            aws_access_key_id = {{ aws_access_key_id }}
            aws_secret_access_key = {{ aws_secret_access_key }}

    - name: Call Role to destroy the OpenShift cluster
      include_role:
        name: host-ocp4-destroy
      when:
      - bwait is successful

- name: Destroy AWS Resources
  hosts: localhost
  tasks:
  - name: Call AWS Destroy Role
    when: cloud_provider == 'ec2'
    include_role:
      name: infra-ec2-template-destroy

- name: Import default azure destroy playbook
  when:
  - cloud_provider == 'azure' or
    cloud_provider == 'gcp'
  import_playbook: "../../cloud_providers/{{ cloud_provider }}_destroy_env.yml"
