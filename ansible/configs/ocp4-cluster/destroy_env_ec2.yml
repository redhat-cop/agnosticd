---
- name: Destroy environment on AWS
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  environment:
    AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
    AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
    AWS_DEFAULT_REGION: "{{aws_region_final|d(aws_region)}}"
  tasks:
  - name: Get fact for cloudformation stack
    cloudformation_info:
      stack_name: "{{ project_tag }}"
    register: stack_facts

  - name: Grab and set stack creation time
    when: project_tag in stack_facts.cloudformation
    vars:
      _stack_description: "{{ stack_facts.cloudformation[project_tag].stack_description }}"
    set_fact:
      stack_creation_time: >-
        {{ _stack_description.creation_time | default(_stack_description.CreationTime) }}
      stack_status: >-
        {{ _stack_description.stack_status | default(_stack_description.StackStatus) }}

  - name: Run infra-ec2-create-inventory role
    include_role:
      name: infra-ec2-create-inventory

  - name: Create local ssh provision facts (key already exists)
    include_role:
      name: create_ssh_provision_key
    when:
    - ssh_provision_key_name is undefined

  - name: SSH config setup
    when:
    - groups["bastions"] is defined
    - groups["bastions"] | length > 0
    include_role:
      name: infra-common-ssh-config-generate

- name: Set ssh extra args for all hosts, use ssh_config just created
  hosts: all
  gather_facts: false
  any_errors_fatal: true
  ignore_errors: false
  tasks:
  - name: add -F option ansible_ssh_extra_args
    set_fact:
      ansible_ssh_extra_args: "{{ ansible_ssh_extra_args|d() }} -F {{ hostvars['localhost'].ansible_ssh_config }}"

- name: Start all EC2 instances if they are stopped
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  environment:
    AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
    AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
    AWS_DEFAULT_REGION: "{{aws_region_final|d(aws_region)}}"
  tasks:
  - include_tasks: ec2_instances_start.yaml

# Call Remove Workloads for workloads that need to clean up "other" infrastructure.
# Workloads that need to be able to be run on the provisioning host (aka not a Bastion)
- name: Remove workloads
  hosts: localhost
  connection: local
  gather_facts: false
  run_once: true
  become: false
  tasks:
  - name: Remove ocp workloads
    when:
    - remove_workloads | default("") | length > 0
    block:
    - name: Invoke roles to remove ocp workloads
      include_role:
        name: "{{ workload_loop_var }}"
      vars:
        ocp_username: "system:admin"
        ACTION: "remove"
        silent: false
      loop: "{{ remove_workloads }}"
      loop_control:
        loop_var: workload_loop_var

# Call Remove Workloads for workloads that need to clean up "other" infrastructure.
# Workloads that need to be run on the bastion
- name: Remove workloads
  hosts: bastions
  gather_facts: false
  run_once: true
  become: false
  tasks:
  - name: Check if k8s interpreter venv is installed
    ansible.builtin.stat:
      path: /opt/virtualenvs/k8s/bin/python
    register: interpreter_bin

  - name: Remove workloads
    when: interpreter_bin.stat.exists
    block:
    - name: Set Ansible Python interpreter to k8s virtualenv
      set_fact:
        ansible_python_interpreter: /opt/virtualenvs/k8s/bin/python

    - name: Remove ocp workloads on bastion
      when:
      - remove_workloads_bastion | default("") | length > 0
      block:
      - name: Invoke roles to remove ocp workloads
        include_role:
          name: "{{ workload_loop_var }}"
        vars:
          ocp_username: "system:admin"
          ACTION: "remove"
          silent: false
        loop: "{{ remove_workloads_bastion }}"
        loop_control:
          loop_var: workload_loop_var

- name: Have the OpenShift installer cleanup what it did
  hosts: bastions
  gather_facts: false
  become: false
  tasks:
  - name: Wait for the bastion host to be available
    wait_for_connection:
      timeout: 60
    register: bwait
    ignore_errors: true

  - name: Ensure old AWS credentials are absent
    file:
      state: absent
      path: "/home/{{ ansible_user }}/.aws/credentials"

  - name: Add AWS credentials for sandboxes
    when:
    - hostvars.localhost.student_access_key_id | default("") | length > 0
    - hostvars.localhost.student_access_secret_key | default("") | length > 0
    become: false
    blockinfile:
      path: "/home/{{ ansible_user }}/.aws/credentials"
      create: true
      insertbefore: BOF
      marker: "# {mark} ANSIBLE MANAGED BLOCK Student Credentials"
      block: |-
        [default]
        aws_access_key_id = {{ hostvars.localhost.student_access_key_id }}
        aws_secret_access_key = {{ hostvars.localhost.student_secret_access_key }}

  # Use shared account when no sandbox account has been configured
  - name: Add AWS credentials for shared account
    when:
    - hostvars.localhost.student_access_key_id | default("") | length == 0
    - aws_access_key_id | default("") != ""
    - aws_secret_access_key | default("") != ""
    become: false
    blockinfile:
      state: present
      path: "/home/{{ ansible_user }}/.aws/credentials"
      create: true
      insertbefore: BOF
      marker: "# {mark} ANSIBLE MANAGED BLOCK Default Credentials"
      block: |-
        [default]
        aws_access_key_id = {{ aws_access_key_id }}
        aws_secret_access_key = {{ aws_secret_access_key }}

  - name: Call role to destroy the OpenShift cluster
    when: bwait is successful
    include_role:
      name: host-ocp4-destroy

  - name: Remove ROSA clusters via CLI
    when:
    - rosa_delete_clusters_cli_enable | default(true) | bool
    include_role:
      name: rosa-delete-clusters-cli

- name: Import default cloud provider destroy playbook
  import_playbook: "../../cloud_providers/{{ cloud_provider }}_destroy_env.yml"
