###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

### Common Host settings
repo_method: file # Other Options are: file, satellite and rhn
# Do you want to run a full yum update
update_packages: true
#If using repo_method: satellite, you must set these values as well.
# satellite_url: satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"

## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another "just like it"
guid: defaultguid

# The next flag is 1 by default. If it is set to more than 1 then instead of creating
# clientvm.guid.baseurl it will create clientvm{1..num_users}.guid.baseurl
num_users: 1
install_bastion: true
install_common: true
install_opentlc_integration: true
install_ipa_client: false
# Install a user id 'student'. If install_student_user=true then a global variable
# student_password=password needs to be provided with the password to set for the user student
install_student_user: false
install_ftl: false

### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
# if you set "use_own_key" to false your PRIVATE key will be copied to the bastion. (This is {{key_name}})

use_own_key: true
env_authorized_key: "{{guid}}key"
ansible_ssh_private_key_file: ~/.ssh/{{key_name}}.pem
set_env_authorized_key: true

# Is this running from Red Hat Ansible Tower
tower_run: false

### AWS EC2 Environment settings

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will create your Public DNS entries
# This only needs to be defined if your CF template uses route53
HostedZoneId: Z3IHLWJZOU9SRT
# The region to be used, if not specified by -e in the command line
aws_region: us-east-1
# The key that is used to
key_name: "default_key_name"

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

## Environment Sizing

clientvm_instance_type: "t2.large"
clientvm_instance_image: RHELAMI

###### VARIABLES YOU SHOULD ***NOT*** CONFIGURE FOR YOUR DEPLOYEMNT

## This might get removed
env_specific_images:
#   - "registry.access.redhat.com/jboss-eap-7/eap70-openshift:latest"
#   - "registry.access.redhat.com/openshift3/jenkins-2-rhel7:latest"
#   - "registry.access.redhat.com/openshift3/jenkins-slave-maven-rhel7:latest"

#### Vars for the OpenShift Ansible hosts file
## TODO: This should be registered as a variable. Awk for os verions (OCP).
## yum info openshift...
osrelease: 3.11.154
repo_version: "{{ osrelease | regex_replace('^([0-9])\\.([0-9]*).*', '\\1.\\2')  }}"

docker_device: /dev/xvdb
docker_version: "{{ '1.12.6' if repo_version is version_compare('3.9', '<')  else '1.13.1' }}"
docker_size: 200

###### You can, but you usually wouldn't need to.
ansible_user: ec2-user
remote_user: ec2-user

common_packages:
  - python
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - nano
  - ansible
  - git
  - vim-enhanced
  - at
  - sysstat
  - strace
  - net-tools
  - iptables-services
  - bridge-utils
  - kexec-tools
  - sos
  - psacct

rhel_repos:
  - rhel-7-server-rpms
  - rhel-7-server-extras-rpms
  - "rhel-7-server-ansible-{{ '2.6' if repo_version is version_compare('4.0','<') else '2.7' }}-rpms"
  - rhel-7-server-ose-{{repo_version}}-rpms

# use_subscription_manager: false
# use_own_repos: true
#
rhn_pool_id_string: OpenShift Container Platform

cloud_tags:
  env_type: "{{ env_type }}"
  guid: "{{ guid }}"
  course_name: "{{ course_name | d('unknown') }}"
  platform: "{{ platform | d('unknown') }}"

### CLOUDFORMATIONS vars

project_tag: "{{ env_type }}-{{ guid }}"

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"


bastion_public_dns: "bastion.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
vpcid_name_tag: "{{subdomain_base}}"

az_1_name: "{{ aws_region }}a"
az_2_name: "{{ aws_region }}b"

subnet_private_1_cidr_block: "192.168.2.0/24"
subnet_private_1_az: "{{ az_2_name }}"
subnet_private_1_name_tag: "{{subdomain_base}}-private"

subnet_private_2_cidr_block: "192.168.1.0/24"
subnet_private_2_az: "{{ az_1_name }}"
subnet_private_2_name_tag: "{{subdomain_base}}-private"

subnet_public_1_cidr_block: "192.168.10.0/24"
subnet_public_1_az: "{{ az_1_name }}"
subnet_public_1_name_tag: "{{subdomain_base}}-public"

subnet_public_2_cidr_block: "192.168.20.0/24"
subnet_public_2_az: "{{ az_2_name }}"
subnet_public_2_name_tag: "{{subdomain_base}}-public"

dopt_domain_name: "{{ aws_region }}.compute.internal"

rtb_public_name_tag: "{{subdomain_base}}-public"
rtb_private_name_tag: "{{subdomain_base}}-private"

cf_template_description: "{{ env_type }}-{{ guid }} template "

cloudformation_retries: 2
ocp_report: false

rootfs_size_clientvm: 200

instances:
  - name: "clientvm"
    count: "{{ num_users }}"
    public_dns: true
    floating_ip: true
    image_id: "{{ clientvm_instance_image }}"
    flavor:
      ec2: "{{ clientvm_instance_type }}"
      osp: "{{ clientvm_instance_type }}"
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_clientvm }}"
    volumes:
      - device_name: "{{docker_device}}"
        volume_size: "{{ docker_size }}"
        volume_type: gp2
    security_groups:
      - BastionSG

#### OSP ####
# See cloud_providers/osp_default_vars.yml
# See roles/infra-osp-project-create/defaults/main.yml

# Set this to true if you need to create a new project in OpenStack
# This should almost always be set to true for OpenShift installations
# If it is set to false, the {{ osp_project_name }} must already exist and
# should be able to run whatever you are deploying
osp_project_create: true

# If osp_project_create is set to yes, define those:
# Quotas to set for new project that is created
quota_num_instances: 2
quota_num_cores: 10
quota_memory: 10240 # in MB
quota_num_volumes: 5
quota_volumes_gigs: 500
#quota_loadbalancers: #when Octavia is available
#quota_pool: #when Octavia is available
#quota_networks: 3
#quota_subnets: 3
#quota_routers: 3
quota_fip: 1
quota_sg: 5
#quota_sg_rules: 100
