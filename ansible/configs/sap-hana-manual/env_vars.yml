### Vars that can be removed:
# use_satellite: true
# use_subscription_manager: false
# use_own_repos: false

#If using repo_method: satellite, you must set these values as well.
# satellite_url: satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"

# RHSM
# repo_method: rhn
# use_subscription_manager: true
# rhel_subscription_user: "xxxxxxxxxxxxx"
# rhel_subscription_pass: "xxxxxxxxxxx"
# rhsm_pool_ids:
#   - "xxxxxxxxxxxxxxxxxxxxxx"
#   - "xxxxxxxxxxxxxxxxxxxxxx"
# rhel_repos:
#    - "rhel-8-for-x86_64-baseos-rpms"
#    - "rhel-8-for-x86_64-appstream-rpms"
#    - "ansible-2-for-rhel-8-x86_64-rpms"
# rhel_repos:
#   - "rhel-7-server-rpms"
#   - "rhel-server-rhscl-7-rpms"
#   - "rhel-7-server-ansible-2.8-rpms"

###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command


### Common Host settings

# repo_method: file # Other Options are: file, satellite and rhn

install_ipa_client: false

install_student_user: false
# Do you want to run a full yum update
update_packages: false


## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another "just like it"
guid: defaultguid

# This var is used to identify stack (cloudformation, azure resourcegroup, ...)
project_tag: "{{ env_type }}-{{ guid }}"

# This is where the ssh_config file will be created, this file is used to
# define the communication method to all the hosts in the deployment
deploy_local_ssh_config_location: "{{output_dir}}/"

install_bastion: true
install_common: true
## SB Don't set software_to_deploy from here, always use extra vars (-e) or "none" will be used
#software_to_deploy:: none


### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
# if you set "use_own_key" to false your PRIVATE key will be copied to the bastion. (This is {{key_name}})
use_own_key: true
env_authorized_key: "{{guid}}key"
ansible_ssh_private_key_file: ~/.ssh/{{key_name}}.pem
set_env_authorized_key: true

# Is this running from Red Hat Ansible Tower
tower_run: false

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

## Environment Sizing

bastion_instance_type: "2-16"
bastion_instance_count: 1
rootfs_size_bastion: 50
nfs_size_bastion: 100

hana_instance_type: "4-32"
support_instance_type: "{{ support_instance_type }}"

hana_instance_count: 1
rootfs_size_hana: 100
pv_size_hana: 200

hana_instance_image: "{{ hana_instance_image }}"
support_instance_image: "{{ support_instance_image }}"

# Instances images to use
#bastion_instance_image: xxxxxxxxxxxxxxxx
#hana_instance_image: xxxxxxxxxxxxx

#NFS Server variables
install_nfs: true
nfs_device: "/dev/vdb"
nfs_vg: "nfs"
nfs_export_path: "/srv/nfs"
nfs_shares:
  - "hana"
nfs_exports_config: "*(insecure,rw,root_squash,no_wdelay,sync)"

security_groups:
  - name: HanaSG
    rules:
      - name: HanaSSHPrivate
        description: "SSH private"
        from_port: 22
        to_port: 22
        protocol: tcp
        cidr: "0.0.0.0/0"
        rule_type: Ingress
        group: BastionSG
      - name: HanaUDPPortsPrivate
        description: "Only from bastion"
        from_port: 1
        to_port: 65535
        protocol: udp
        group: BastionSG
        rule_type: Ingress
      - name: HanaTCPPPortsPrivate
        description: "Only from bastion"
        from_port: 1
        to_port: 65535
        protocol: tcp
        group: BastionSG
        rule_type: Ingress
  - name: BastionSG
    rules:
      - name: BastionUDPPortsPrivate
        description: "Only from bastion"
        from_port: 1
        to_port: 65535
        protocol: udp
        group: HanaSG
        rule_type: Ingress
      - name: BastionTCPPPortsPrivate
        description: "Only from bastion"
        from_port: 1
        to_port: 65535
        protocol: tcp
        group: HanaSG
        rule_type: Ingress

instances:
  - name: "bastion"
    count: "{{ bastion_instance_count }}"
    unique: true
    public_dns: true
    dns_loadbalancer: true
    floating_ip: true
    image_id: "{{ bastion_instance_image }}"
    flavor:
      osp: "{{ bastion_instance_type }}"
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
      - key: "ostype"
        value: "linux"
      - key: "instance_filter"
        value: "{{ env_type }}-{{ email }}"
    rootfs_size: "{{ rootfs_size_bastion }}"
    volumes:
      - volume_name: "bastion_nfs"
        volume_size: "{{ nfs_size_bastion }}"
    security_groups:
      - BastionSG

  - name: "hana"
    count: "{{ hana_instance_count }}"
    public_dns: false
    dns_loadbalancer: false
    floating_ip: false
    image_id: "{{ hana_instance_image }}"
    flavor:
      osp: "{{ hana_instance_type }}"
    tags:
      - key: "AnsibleGroup"
        value: "hanas"
      - key: "ostype"
        value: "rhel"
      - key: "instance_filter"
        value: "{{ env_type }}-{{ email }}"
    key_name: "{{key_name}}"
    rootfs_size: "{{ rootfs_size_hana }}"
    volumes:
      - volume_name: "hana_pv"
        volume_size: "{{ pv_size_hana }}"
    security_groups:
      - HanaSG



common_packages:
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - git
  - vim-enhanced
  - at

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"

bastion_public_dns: "bastion.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
vpcid_cidr_block: "192.168.0.0/16"
vpcid_name_tag: "{{subdomain_base}}"


#dopt_domain_name: "{{ aws_region }}.compute.internal"

rtb_public_name_tag: "{{subdomain_base}}-public"
rtb_private_name_tag: "{{subdomain_base}}-private"


cf_template_description: "{{ env_type }}-{{ guid }} Ansible Agnostic Deployer "

#### OSP ####
# See cloud_providers/osp_default_vars.yml
# See roles/infra-osp-project-create/defaults/main.yml

# Set this to true if you need to create a new project in OpenStack
# This should almost always be set to true for OpenShift installations
# If it is set to false, the {{ osp_project_name }} must already exist and
# should be able to run whatever you are deploying
#osp_project_create: true

# If osp_project_create is set to yes, define those:
# Quotas to set for new project that is created
#quota_num_instances: 15
#quota_num_cores: 72
#quota_memory: 131072 # in MB
#quota_num_volumes: 25
#quota_volumes_gigs: 500
#quota_loadbalancers: #when Octavia is available
#quota_pool: #when Octavia is available
#quota_networks: 3
#quota_subnets: 3
#quota_routers: 3
quota_fip: 7
#quota_sg: 10
#quota_sg_rules: 100
