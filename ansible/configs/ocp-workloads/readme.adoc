:toc2:

= Deploy OpenShift workloads to a cluster or clusters

This config is used to run OpenShift workloads on a single cluster or a list of clusters.

It is a config like any other agnosticd config and allows integration of this particular kind of deployment into our eco-system.

It can access a single cluster via a bastion or multiple clusters by direct connection the the cluster apis.


== Create


.Example: `using-bastion.yml`
[source,yaml]
----
cloud_provider: none
env_type: ocp-workloads

ocp_workloads:
  - ocp4-workload-fuse-ignite

target_host: bastion.dev.mycluster.mydomain.com

#become_override: false

# If the ocp-workload supports it, you should specify the OCP user:
# ocp_username: myuser

# Usually the ocp-workloads want a GUID also:
# guid: changeme
----

.Example: `multicluster-bastion.yml`
----
cloud_provider: none
guid: changeme
env_type: ocp-workloads

cluster_workloads:
- name: ocp4_workload_authentication
  clusters:
  - openshift_app
  - openshift_db
- name: ocp4_workload_mitzi_app
  clusters:
  - openshift_app
- name: ocp4_workload_mitzi_db
  clusters:
  - openshift_db

openshift_app:
  api_ca_cert: ...
  api_key: ...
  api_url: ...

openshift_db:
  api_ca_cert: ...
  api_key: ...
  api_url: ...
----


.Run
[source,shell]
----
cd agnosticd/ansible
ansible-playbook main.yml -e @vars.yml
----

== Delete

Just run the following:

[source,shell]
----
cd agnosticd/ansible
ansible-playbook destroy.yml -e @vars.yml
----

It will run the ocp-workload role with `ACTION=destroy`.


== Lifecycle

The link:../../lifecycle_entry_point.yml[`lifecycle_entry_point.yml`] playbook can be used as well.

It will just run the workload passing the `ACTION` variable. Just make sure  to implement the action `stop`, `start`, `status` in the ocp-workloads.

== `cluster_workloads` variable

This variable allows configuration to run workloads across multiple clusters.
This feature is currently only supported with direct connection from localhost.

The `cluster_workloads` array must specify target `clusters` whose names correspond to variables than hold connection configuration for the cluster as shown in the example above.


== `target_host` variable

This variable correspond to run workloads on a single cluster using a bastion host to run `oc` commands or `k8s` type ansible tasks.

You can specify the target host in three different ways.

=== As localhost

[source,yaml]
----
target_host: localhost
----

This will execute the workload from localhost.
It requires that whatever host the playbook is run from is authenticated to the OpenShift cluster.

=== As an hostname

If you want to just specify:

[source,yaml]
----
target_host: bastion.dev.mycluster.mydomain.com
----

Then you need to configure ssh properly to be able to connect to that host.
Just make sure the command `ssh bastion.dev.mycluster.mydomain.com` works.

=== As a dictionary

You can specify the bastion host using a dictionary. This is useful is you need to specify the user, port, ssh_key to use, etc.

[source,yaml]
----
target_host:
  ansible_host: bastion.babydev.babylon.open.redhat.com
  ansible_port: 22
  ansible_user: ec2-user
  #ansible_ssh_private_key_content: "{{ ssh_private_key_content }}"
  ansible_ssh_private_key_file: ~/.ssh/admin_key.pem
  #ansible_ssh_extra_args:  ...
----

NOTE: you can add the `ansible_ssh_private_key_content` to a secret file or a vault. The config will create the key using that content in the directory `output_dir/` and use it to connect to the bastion. The key will then be deleted when the playbook ends, see link:cleanup.yml[`cleanup.yml`].


== FAQ

. But i want to run my workload as root on the bastion!

Just use the var `become_override`. Set it to true in your var file. Most ocp-workloads implement that variable.

.extract of `main.yml` in ocp-workload
[source,yaml]
----
- name: Running Workload Tasks
  import_tasks: ./workload.yml
  become: "{{ become_override | bool }}"
  when: ACTION == "create" or ACTION == "provision"
----
