---
- name: Destroy environment on AWS
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  environment:
    AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
    AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
    AWS_DEFAULT_REGION: "{{aws_region_final|d(aws_region)}}"
  tasks:
  - name: Create infra key
    include_role:
      name: infra-ec2-ssh-key
    when:
    - install_infra_ssh_key | default(false) | bool

  - name: Get fact for cloudformation stack
    cloudformation_facts:
      stack_name: "{{ project_tag }}"
    register: stack_facts

  - name: Grab and set stack creation time
    when: project_tag in stack_facts.ansible_facts.cloudformation
    vars:
      _stack_description: "{{ stack_facts.ansible_facts.cloudformation[project_tag].stack_description }}"
    set_fact:
      stack_creation_time: >-
        {{ _stack_description.creation_time | default(_stack_description.CreationTime) }}
      stack_status: >-
        {{ _stack_description.stack_status | default(_stack_description.StackStatus) }}

  - name: Run infra-ec2-create-inventory role
    include_role:
      name: infra-ec2-create-inventory
  - name: SSH config setup
    when:
    - groups["bastions"] is defined
    - groups["bastions"] | length > 0
    include_role:
      name: infra-common-ssh-config-generate

- name: Set ssh extra args for all hosts, use ssh_config just created
  hosts: all
  gather_facts: false
  any_errors_fatal: true
  ignore_errors: false
  tasks:
  - name: add -F option ansible_ssh_extra_args
    set_fact:
      ansible_ssh_extra_args: "{{ ansible_ssh_extra_args|d() }} -F {{ hostvars['localhost'].ansible_ssh_config }}"

- name: Start all EC2 instances if they are stopped
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  environment:
    AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
    AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
    AWS_DEFAULT_REGION: "{{aws_region_final|d(aws_region)}}"
  tasks:
  - name: Get all EC2 instances
    ec2_instance_info:
      filters:
        "tag:guid": "{{ guid }}"
        "tag:env_type": "{{ env_type }}"
        instance-state-name: stopped
    register: r_stopped_instances

  - name: Ensure EC2 instances are running
    when: r_stopped_instances.instances | length > 0
    ec2_instance:
      instance_ids: "{{ item.instance_id }}"
      state: started
      wait: false
    loop: "{{ r_stopped_instances.instances }}"

  - name: Wait until all EC2 instances are running
    when: r_stopped_instances.instances | length > 0
    ec2_instance_info:
      filters:
        "tag:guid": "{{ guid }}"
        "tag:env_type": "{{ env_type }}"
        instance-state-name: running
    register: r_running_instances
    until: r_running_instances.instances | length | int >= r_stopped_instances.instances | length | int
    delay: 10
    retries: 60

- name: Destroy ROSA
  hosts: bastions
  gather_facts: false
  become: false
  environment:
    AWS_DEFAULT_REGION: "{{ aws_region }}"
  tasks:
  - set_fact:
      rosa_cluster_name: "rosa-{{ guid }}"
  - name: Destroy ROSA Cluster
    command: "/usr/local/bin/rosa delete cluster -y --cluster={{ rosa_cluster_name }}"

  - name: Wait for ROSA deletion to complete
    command: "/usr/local/bin/rosa describe cluster -c {{ rosa_cluster_name }}"
    register: rosa_cluster_status
    ignore_errors: true
    until: rosa_cluster_status.rc != 0
    retries: 60
    delay: 60

  - name: Make sure ROSA cluster is gone
    fail:
      msg: "The ROSA cluster still exists after one hour of trying to delete.  Please look at it manually."
    when: rosa_cluster_status.rc == 0

- name: Import cloud provider specific destroy playbook
  import_playbook: "../../cloud_providers/{{ cloud_provider }}_destroy_env.yml"
