---
## TODO: What variables can we strip out of here to build complex variables?
## i.e. what can we add into group_vars as opposed to config_vars?
## Example: We don't really need "subdomain_base_short". If we want to use this,
## should just toss in group_vars/all.
### Also, we should probably just create a variable reference in the README.md
### For now, just tagging comments in line with configuration file.

### Vars that can be removed:
# use_satellite: true
# use_subscription_manager: false
# use_own_repos: false

###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

### Common Host settings
repo_method: file # Other Options are: file, satellite and rhn
osrelease: 3.11.59
repo_version: "{{ osrelease | regex_replace('^([0-9])\\.([0-9]*).*', '\\1.\\2') }}"

### Authentication for Red Hat Registry (registry.redhat.io), needs to be a user that can log into
### the customer portal. Not required before 3.11...
### Best to set via the command line
redhat_registry_user: ""
redhat_registry_password: ""

#If using repo_method: satellite, you must set these values as well.
# satellite_url: satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"

# Do you want to run a full yum update
update_packages: true

## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another "just like it"
guid: defaultguid

# This var is used to identify stack (cloudformation, azure resourcegroup, ...)
project_tag: "{{ env_type }}-{{ guid }}"

cloud_tags: "{ 'env_type': '{{ env_type }}', 'guid': '{{ guid }}', 'course_name': '{{ course_name }}', 'platform': '{{ platform }}' }"

## SB Don't set software_to_deploy from here, always use extra vars (-e) or "none" will be used
#software_to_deploy: openshift
deploy_openshift: true
deploy_openshift_post: true
deploy_env_post: true

install_bastion: true
install_student_user: false
install_common: true
install_nfs: true
install_dynamic_nfs: false
install_glusterfs: false
install_opentlc_integration: true
install_zabbix: false
install_prometheus: false
install_ipa_client: false
install_openwhisk: false
install_metrics: true
install_logging: true
install_aws_broker: false
install_nexus: true
install_openshiftapb: false
install_maistra: false
install_lets_encrypt_certificates: false
# Set the next variable to false to run tests.
#   This prevents hitting the rate limiter of
#   Let's Encrypt when requesting lots of certificates
#   Set to true for "real" certificates
#   No effect when install_lets_encrypt_certificates=false
lets_encrypt_production: true

glusterfs_device_name: /dev/xvdc
glusterfs_device_size: 1500
glusterfs_image_tag: v3.11

ocp_report: false
remove_self_provisioners: false
idm_ca_url: http://ipa.opentlc.com/ipa/config/ca.crt
zabbix_host: 23.246.247.58

# Options for container_runtime: docker, cri-o
container_runtime: "docker"
docker_version: "{{ '1.12.6' if repo_version is version_compare('3.9', '<')  else '1.13.1' }}"
docker_device: /dev/xvdb

### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
# if you set "use_own_key" to false your PRIVATE key will be copied to the bastion. (This is {{key_name}})

use_own_key: true
env_authorized_key: "{{guid}}key"
ansible_ssh_private_key_file: ~/.ssh/{{key_name}}.pem
set_env_authorized_key: true

# Is this running from Red Hat Ansible Tower
tower_run: false

admin_user: opentlc-mgr
admin_project: "ocp-workshop"

#Â UI Customizations
enable_workshops_catalog: false

# Red Hat Registry Location
redhat_registry: "{{ 'registry.access.redhat.com' if repo_version is version_compare('3.11', '<')  else 'registry.redhat.io' }}"

### Azure

# Create a dedicated resourceGroup for this deployment
az_destroy_method: resource_group
az_resource_group: "{{ project_tag }}"

# you can operate differently: if you share on resourceGroup for all you deployments,
# you can specify a different resourceGroup and method:
#az_destroy_method: deployment
#az_resource_group: my-shared-resource-group
#az_storage_account_type: Premium_LRS

### AWS EC2 Environment settings

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will create your Public DNS entries
# This only needs to be defined if your CF template uses route53
HostedZoneId: Z1TQFSYFZUAO0D
# The region to be used, if not specified by -e in the command line
aws_region: us-east-1
# The key that is used to
key_name: "default_key_name"

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".openshift.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

## Environment Sizing

bastion_instance_type: "t2.large"
master_instance_type: "m4.4xlarge"
etcd_instance_type: "{{master_instance_type}}"
infranode_instance_type: "m4.4xlarge"
node_instance_type: "m4.4xlarge"
support_instance_type: "m4.4xlarge"
nfs_instance_type: "{{ support_instance_type }}"

node_instance_count: 5
infranode_instance_count: 1
master_instance_count: 1
support_instance_count: "{{ 3 if install_glusterfs|bool else 1 }}"
# scaleup
new_node_instance_count: 0

###### VARIABLES YOU SHOULD ***NOT*** CONFIGURE FOR YOUR DEPLOYEMNT
timeout: 60

## This might get removed
env_specific_images:
#  - "{{ redhat_registry }}/openshift3/jenkins-2-rhel7:v{{ repo_version }}"
#  - "{{ redhat_registry }}/openshift3/ose-recycler:v{{ repo_version }}"

#### Vars for the OpenShift Ansible hosts file
master_api_port: 443
ovs_plugin: "subnet" # This can also be set to: "multitenant" or "networkpolicy"
multi_tenant_setting: "os_sdn_network_plugin_name='redhat/openshift-ovs-{{ovs_plugin}}'"
master_lb_dns: "master.{{subdomain_base}}"
master_lb_private_dns: "master1.{{ guid }}.internal"

openshift_master_overwrite_named_certificates: true

lets_encrypt_openshift_master_named_certificates:
  - certfile: "/root/certificates/fullchain.pem"
    keyfile: "/root/certificates/key.pem"
    cafile: "/root/certificates/ca.cer"

lets_encrypt_openshift_hosted_router_certificate:
  certfile: "/root/certificates/fullchain.pem"
  keyfile: "/root/certificates/key.pem"
  cafile: "/root/certificates/ca.cer"

project_request_message: 'To provision Projects you must request access in https://labs.opentlc.com or https://rhpds.redhat.com'

cloudapps_suffix: 'apps.{{subdomain_base}}'

########## OCP identity providers
# Options for install_idm: allow_all, htpasswd, ldap, ...  see the available below
install_idm: htpasswd

# if you want to install several identity providers, just pick from the
# available_identity_providers list:
install_idms:
  - "{{ install_idm }}"

# This var is empty by default.
# Every idm in the list 'install_idms' will be added, using the 'available_identity_providers' map
# you can:
#   - directly override the 'identity_providers' list
# or
#   - add an option to 'available_identity_providers' and then
#     reference it in 'install_idm' or the 'install_idms' list
identity_providers: []

openshift_master_ldap_ca_file: 'openshift_master_ldap_ca_file=/root/ca.crt'

available_identity_providers:
  ldap:
    name: 'OpenTLC IPA'
    challenge: 'true'
    login: 'true'
    kind: 'LDAPPasswordIdentityProvider'
    attributes:
      id: ['dn']
      email: ['mail']
      name: ['cn']
      preferredUsername: ['uid']
    bindDN: 'uid=ose-mwl-auth,cn=users,cn=accounts,dc=opentlc,dc=com'
    bindPassword: "{{bindPassword|d('NOT_DEFINED')}}"
    ca: ipa-ca.crt
    insecure: 'false'
    url: ldaps://ipa1.opentlc.com:636/cn=users,cn=accounts,dc=opentlc,dc=com?uid

  ssodev:
    name: ssodev-iad00
    challenge: 'false'
    login: 'true'
    kind: OpenIDIdentityProvider
    clientID: "{{ opentlc_ssodev_client_id|d('NOT_DEFINED') }}"
    clientSecret: "{{ opentlc_ssodev_client_secret|d('NOT_DEFINED') }}"
    ca: lets-encrypt-x3-cross-signed.pem.txt
    urls:
      authorize: https://ssodev-iad00.opentlc.com:8443/auth/realms/ipatest/protocol/openid-connect/auth
      token: https://ssodev-iad00.opentlc.com:8443/auth/realms/ipatest/protocol/openid-connect/token
      userInfo: https://ssodev-iad00.opentlc.com:8443/auth/realms/ipatest/protocol/openid-connect/userinfo
    claims:
      id:
        - sub
      preferredUsername:
        - preferred_username
      name:
        - name
      email:
        - email

  allow_all:
    name: allow_all
    login: 'true'
    challenge: 'true'
    kind: AllowAllPasswordIdentityProvider

  htpasswd:
    name: htpasswd_auth
    login: 'true'
    challenge: 'true'
    kind: HTPasswdPasswordIdentityProvider
    filename: /etc/origin/master/htpasswd

admission_plugin_config:
  MutatingAdmissionWebhook:
    configuration:
      apiVersion: apiserver.config.k8s.io/v1alpha1
      kubeConfigFile: /dev/null
      kind: WebhookAdmission
  ValidatingAdmissionWebhook:
    configuration:
      apiVersion: apiserver.config.k8s.io/v1alpha1
      kubeConfigFile: /dev/null
      kind: WebhookAdmission

###### You can, but you usually wouldn't need to.
ansible_user: ec2-user
remote_user: ec2-user

common_packages:
  - python
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - nano
  - ansible
  - git
  - vim-enhanced
  - at
  - sysstat
  - strace
  - net-tools
  - iptables-services
  - bridge-utils
  - kexec-tools
  - sos
  - psacct
  - iotop
  - rsync

rhel_repos:
  - rhel-7-server-rpms
  - rhel-7-server-extras-rpms
  - rhel-7-server-ose-{{repo_version}}-rpms

# use_subscription_manager: false
# use_own_repos: true
#
rhn_pool_id_string: OpenShift Container Platform

## NFS Server settings
nfs_vg: nfsvg
nfs_pvs: /dev/xvdd
nfs_export_path: /srv/nfs
nfs_size: 200

nfs_shares:
  - user-vols

ocp_pvs:
#   - nexus3

user_vols: 200
user_vols_size: 10Gi
user_count: 200

# The Logic for the cache_images is disabled in install_docker.yml.... so this has no effect
cache_images:
  - "{{ redhat_registry }}/openshift3/jenkins-2-rhel7:v{{ repo_version }}"
  - "{{ redhat_registry }}/openshift3/ose-recycler:v{{ repo_version }}"

### CLOUDFORMATIONS vars

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"

cloudapps_record: '*.apps'
cloudapps_dns: '{{cloudapps_record}}.{{subdomain_base}}.'

master_public_dns: "master.{{subdomain_base}}."
bastion_public_dns: "bastion.{{subdomain_base}}."
certtest_public_dns: "certtest.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
vpcid_cidr_block: "192.168.0.0/16"
vpcid_name_tag: "{{subdomain_base}}"

az_1_name: "{{ aws_region }}a"
az_2_name: "{{ aws_region }}b"

subnet_private_1_cidr_block: "192.168.2.0/24"
subnet_private_1_az: "{{ az_2_name }}"
subnet_private_1_name_tag: "{{subdomain_base}}-private"

subnet_private_2_cidr_block: "192.168.1.0/24"
subnet_private_2_az: "{{ az_1_name }}"
subnet_private_2_name_tag: "{{subdomain_base}}-private"

subnet_public_1_cidr_block: "192.168.10.0/24"
subnet_public_1_az: "{{ az_1_name }}"
subnet_public_1_name_tag: "{{subdomain_base}}-public"

subnet_public_2_cidr_block: "192.168.20.0/24"
subnet_public_2_az: "{{ az_2_name }}"
subnet_public_2_name_tag: "{{subdomain_base}}-public"

dopt_domain_name: "{{ aws_region }}.compute.internal"

rtb_public_name_tag: "{{subdomain_base}}-public"
rtb_private_name_tag: "{{subdomain_base}}-private"

cf_template_description: "{{ env_type }}-{{ guid }} template "

rootfs_size_node: 50
rootfs_size_infranode: 150
rootfs_size_master: 50
rootfs_size_bastion: 20
rootfs_size_support: 20

instances:
  - name: "bastion"
    count: 1
    unique: true
    public_dns: true
    dns_loadbalancer: true
    flavor:
      ec2: "{{bastion_instance_type}}"
      azure: "{{bastion_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_bastion }}"

  - name: "master"
    count: "{{master_instance_count}}"
    public_dns: true
    dns_loadbalancer: true
    flavor:
      ec2: "{{master_instance_type}}"
      azure: "{{master_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "masters"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_master }}"
    volumes:
      - name: docker_device
        device_name: "{{docker_device}}"
        volume_size: "{{master_docker_size|default(docker_size)|default('20')}}"
        volume_type: gp2
        purpose: docker
        lun: 0

  - name: "node"
    count: "{{node_instance_count}}"
    public_dns: false
    dns_loadbalancer: false
    flavor:
      ec2: "{{node_instance_type}}"
      azure: "{{node_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "nodes"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_node }}"
    volumes:
      - name: docker_device
        device_name: "{{docker_device}}"
        volume_size: "{{node_docker_size|d(docker_size)|d('100')}}"
        volume_type: gp2
        purpose: docker
        lun: 0

  - name: "infranode"
    count: "{{infranode_instance_count}}"
    public_dns: true
    dns_loadbalancer: true
    flavor:
      ec2: "{{infranode_instance_type}}"
      azure: "{{infranode_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "infranodes"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_infranode }}"
    volumes:
      - name: docker_device
        device_name: "{{docker_device}}"
        volume_size: "{{infranode_docker_size|d(docker_size)|d('50')}}"
        volume_type: gp2
        purpose: docker
        lun: 0

  - name: "support"
    count: "{{support_instance_count}}"
    public_dns: '{{ true if install_glusterfs | bool else false }}'
    dns_loadbalancer: false
    flavor:
      ec2: "{{support_instance_type}}"
      azure: "{{support_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "{{ 'support,glusterfs,nodes' if install_glusterfs|bool else 'support' }}"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_support }}"
    volumes:
      - name: docker_device
        device_name: "{{docker_device}}"
        volume_size: "{{support_docker_size|d(docker_size)|d('50')}}"
        volume_type: gp2
        purpose: docker
      - name: glusterfs_device_name
        enable: "{{ install_glusterfs | bool }}"
        device_name: "{{glusterfs_device_name}}"
        volume_size: "{{glusterfs_device_size}}"
        volume_type: gp2
        purpose: glusterfs
      - name: nfs_device
        enable: "{{ install_nfs | bool }}"
        device_name: "{{nfs_pvs}}"
        volume_size: "{{nfs_size}}"
        volume_type: gp2
        purpose: nfs
