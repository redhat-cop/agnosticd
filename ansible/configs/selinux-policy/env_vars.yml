
bastion_instance_type:
  ec2: "t3.medium"
  azure: Standard_A2_V2

bastion_instance_image: RHEL81

node_instance_type:
  ec2: "t3.medium"
  azure: Standard_A2_V2

node_instance_image: RHEL81

# How many do you want for each instance type
node_instance_count: 0

# Environment Instances
instances:
  - name: "bastion"
    count: 1
    unique: true
    public_dns: true
    dns_loadbalancer: false
    image: "{{ bastion_instance_image }}"
    flavor:
      ec2: "t3.medium"
      azure: Standard_A2_V2
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
      - key: "ostype"
        value: "linux"
      - key: "instance_filter"
        value: "{{ env_type }}-{{ email }}"
      - key: ansible_python_interpreter
        value: /usr/libexec/platform-python
    volumes:
      - name: '/dev/sda1'
        size: 20
    security_groups:
      - "BastionSG"

  - name: "node"
    count: "{{node_instance_count}}"
    public_dns: true
    dns_loadbalancer: false
    image: "{{ node_instance_image }}"
    flavor:
      ec2: "t3.medium"
      azure: Standard_A2_V2
    tags:
      - key: "AnsibleGroup"
        value: "nodes"
      - key: "ostype"
        value: "linux"
      - key: "instance_filter"
        value: "{{ env_type }}-{{ email }}"
      - key: ansible_python_interpreter
        value: /usr/libexec/platform-python

# DNS settings for environmnet
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"

# Stuff that only GPTE cares about:
install_ipa_client: false


repo_method: file
repo_version: "3.10"
# Do you want to run a full yum update
update_packages: false
common_packages:
  - python
  - unzip
  - bash-completion
  - tmux
  - wget
  - git
  - vim-enhanced
  - at

rhel_repos:
  - rhel-7-server-rpms
  - rhel-7-server-extras-rpms
  - epel-release-latest-7

###V2WORK, these should just be set as default listed in the documentation
install_bastion: true
install_common: true
install_student_user: true
student_user: lab-user
## SB Don't set software_to_deploy from here, always use extra vars (-e) or "none" will be used
#software_to_deploy: none


## guid is the deployment unique identifier, it will be appended to all tags,
## files and anything that identifies this environment from another.
# Using GUID is required, if it is not passed in the command line or uncommented
# here the deployment will fail
#guid: defaultguid



###V2WORK, these should just be set as default listed in the documentation
# This is where the ssh_config file will be created, this file is used to
# define the communication method to all the hosts in the deployment
deploy_local_ssh_config_location: "{{output_dir}}/"


### If you want a Key Pair name created and injected into the hosts,
# set `set_env_authorized_key` to true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment or use your own self generated key
# if you set "use_own_key" to false your PRIVATE key will be copied to the bastion. (This is {{key_name}})

###V2WORK, these should just be set as default listed in the documentation
use_own_key: true
env_authorized_key: "{{guid}}key"
set_env_authorized_key: true

################################################################################
################################################################################
### AWS EC2 Specific Variables
################################################################################
################################################################################

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will create your Public DNS entries
# This only needs to be defined if your CF template uses route53
HostedZoneId: Z3IHLWJZOU9SRT
# The region to be used, if not specified by -e in the command line
aws_region: ap-southeast-2
# The key that is used to connect to the AWS instance initially, it should
# exist in your aws account and the private key should exist on the local machine
# you are provisioning from.
#key_name: "default_key_name"

###V2WORK THIS SHOULD MOVE INTO THE ROLE
# This var is used to identify stack (cloudformation, azure resourcegroup, ...)
project_tag: "{{ env_type }}-{{ guid }}"

################################################################################
################################################################################
### Azure Specific Variables
################################################################################
################################################################################
# Create a dedicated resourceGroup for this deployment
az_destroy_method: resource_group
az_resource_group: "{{ project_tag }}"

# you can operate differently: if you share on resourceGroup for all you deployments,
# you can specify a different resourceGroup and method:
#az_destroy_method: deployment
#az_resource_group: my-shared-resource-group
