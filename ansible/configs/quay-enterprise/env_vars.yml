###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

### Common Host settings
repo_method: file # Other Options are: file, satellite and rhn
# Do you want to run a full yum update
update_packages: true
# If using repo_method: satellite, you must set these
# values as well.
# satellite_url: https://satellite.example.com
# satellite_org: Sat_org_name
# satellite_activationkey: "rhel7basic"

## guid is the deployment unique identifier, it
## will be appended to all tags, files and anything
## that identifies this environment from another
## "just like it"
guid: defaultguid

install_bastion: true
install_common: true
install_opentlc_integration: true
install_ipa_client: false
install_student_user: true
docker_device: /dev/xvdb
docker_version: '1.13.1'

### Quay Settings
qe_quay_hostname: quay
qe_quay_instance_type: m4.2xlarge

# Quay Database Configuration
qe_quay_database_host: "{{ qe_quay_hostname }}"
qe_quay_database_type: postgresql
qe_quay_database_port: 5432
qe_quay_database_name: quay
qe_quay_database_username: quay
qe_quay_database_password: quay
qe_quay_database_admin_username: root
qe_quay_database_admin_password: quayadmin

qe_quay_ssl_enable: True
qe_quay_ssl_lets_encrypt_certs: False
qe_quay_ssl_lets_encrypt_production: False
qe_quay_ssl_lets_encrypt_force_renew: False
qe_quay_ssl_lets_encrypt_renew_automatically: False
#qe_quay_ssl_key_file: "{{output_dir}}/{{guid}}.key"
#qe_quay_ssl_cert_file: "{{output_dir}}/{{guid}}.cert"
# qe_quay_superuser_username: quayadmin
# qe_quay_superuser_password: quaypwd
# qe_quay_superuser_email: quayadmin@dummy.com

# Clair
qe_quay_clair_enable: False
qe_quay_clair_hostname: clair
qe_quay_clair_instance_type: t2.2xlarge

# Clair Database Configuration
qe_clair_database_name: clair
qe_clair_database_host: "{{ qe_clair_hostname }}"
qe_clair_database_username: clair
qe_clair_database_password: clair
qe_clair_database_port: 5433
qe_clair_database_admin_username: root
qe_clair_database_admin_password: clairadmin

### If you want a Key Pair name created and injected
# into the hosts, set `set_env_authorized_key` to
# true and set the keyname in `env_authorized_key`
# you can use the key used to create the environment
# or use your own self generated key if you set
# "use_own_key" to false your PRIVATE key will be
# copied to the bastion. (This is {{key_name}})

use_own_key: true
env_authorized_key: "{{guid}}key"
ansible_ssh_private_key_file: ~/.ssh/{{key_name}}.pem
set_env_authorized_key: true

# Is this running from Red Hat Ansible Tower
tower_run: false

### AWS EC2 Environment settings

### Route 53 Zone ID (AWS)
# This is the Route53 HostedZoneId where you will
# create your Public DNS entries
# This only needs to be defined if your CF
# template uses route53
HostedZoneId: Z3IHLWJZOU9SRT
# The region to be used, if not specified by
# -e in the command line
aws_region: us-east-1
# The key that is used to
key_name: "default_key_name"

## Networking (AWS)
subdomain_base_short: "{{ guid }}"
subdomain_base_suffix: ".example.opentlc.com"
subdomain_base: "{{subdomain_base_short}}{{subdomain_base_suffix}}"

###### VARIABLES YOU SHOULD ***NOT*** CONFIGURE FOR YOUR DEPLOYEMNT

#### Vars for the repos
osrelease: 3.10.14

###### You can, but you usually wouldn't need to.
ansible_user: ec2-user
remote_user: ec2-user

common_packages:
  - socat
  - iptables-services

rhel_repos:
  - rhel-7-server-rpms
  - rhel-7-server-extras-rpms
  - rhel-7-server-ansible-2.4-rpms
  
# use_subscription_manager: false
# use_own_repos: true
#
# rhn_pool_id_string: OpenShift Container Platform

### CLOUDFORMATIONS vars

project_tag: "{{ env_type }}-{{ guid }}"


zone_internal_dns: "{{guid}}.internal."
chomped_zone_internal_dns: "{{guid}}.internal"


bastion_public_dns: "bastion.{{subdomain_base}}."
bastion_public_dns_chomped: "bastion.{{subdomain_base}}"
vpcid_name_tag: "{{subdomain_base}}"

az_1_name: "{{ aws_region }}a"
az_2_name: "{{ aws_region }}b"

subnet_private_1_cidr_block: "192.168.2.0/24"
subnet_private_1_az: "{{ az_2_name }}"
subnet_private_1_name_tag: "{{subdomain_base}}-private"

subnet_private_2_cidr_block: "192.168.1.0/24"
subnet_private_2_az: "{{ az_1_name }}"
subnet_private_2_name_tag: "{{subdomain_base}}-private"

subnet_public_1_cidr_block: "192.168.10.0/24"
subnet_public_1_az: "{{ az_1_name }}"
subnet_public_1_name_tag: "{{subdomain_base}}-public"

subnet_public_2_cidr_block: "192.168.20.0/24"
subnet_public_2_az: "{{ az_2_name }}"
subnet_public_2_name_tag: "{{subdomain_base}}-public"

dopt_domain_name: "{{ aws_region }}.compute.internal"

rtb_public_name_tag: "{{subdomain_base}}-public"
rtb_private_name_tag: "{{subdomain_base}}-private"

cf_template_description: "{{ env_type }}-{{ guid }} template "

cloudformation_retries: 2
ocp_report: false

rootfs_size_quayvm: 1000
rootfs_size_clairvm: 100

instances:
  - name: "{{ qe_quay_hostname }}"
    count: 1
    public_dns: true
    flavor:
      "ec2": "{{qe_quay_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "bastions,quay_enterprise,quay_database,redis,all_vms"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_quayvm }}"
    volumes:
      - device_name: "{{docker_device}}"
        volume_size: 100
        volume_type: gp2

  - name: "{{ qe_quay_clair_hostname }}"
    count: 1
    public_dns: true
    flavor:
      "ec2": "{{qe_quay_clair_instance_type}}"
    tags:
      - key: "AnsibleGroup"
        value: "clair,clair_database,all_vms"
      - key: "ostype"
        value: "linux"
    rootfs_size: "{{ rootfs_size_clairvm }}"
    volumes:
      - device_name: "{{docker_device}}"
        volume_size: 100
        volume_type: gp2
