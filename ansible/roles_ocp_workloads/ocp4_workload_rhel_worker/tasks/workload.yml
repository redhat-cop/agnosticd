---
# Implement your Workload deployment tasks here

- name: Setting up workload for user
  debug:
    verbosity: 3
    msg: "Setting up workload for user ocp_username = {{ ocp_username }}"

- name: Are there already RHEL_workers?
  ec2_instance_info:
    aws_region: "{{ aws_region }}"
    filters:
      "tag:type": "RHEL_worker"
      "tag:guid": "{{ guid }}"
      instance-state-name: ["running"]
  register: __RHEL_workers

- name: add RHEL_Workers to the inventory
  add_host:
    hostname: "{{ item.public_dns_name }}"
    key_name: "{{ item.key_name }}"
    ansible_user: "ec2-user"
    remote_user: "ec2-user"
    groups: [RHEL_Workers]
    private_dns_name: "{{ item.private_dns_name }}"
    private_ip: "{{ item.private_ip_address }}"
    public_dns_name: "{{ item.public_dns_name }}"
    public_ip: "{{ item.public_ip_address }}"
    tags: "{{ item.tags }}"
  loop: "{{ __RHEL_workers.instances }}"

- name: Create RHEL workers if there arent enough
  when: __RHEL_workers.instances | length < ocp4_workload_rhel_worker_exact_count
  block:
  - name: Find AMI to launch
    ec2_ami_info:
      aws_region: "{{ aws_region }}"
      owners: 309956199498
      filters:
        name: "{{ ocp4_workload_rhel_worker_ami_name }}"
    register: __rhel_ami

  - name: "Get Control Plane Instances for cluster {{ cluster_name }}"
    ec2_instance_info:
      aws_region: "{{ aws_region }}"
      filters:
        "tag:Name": "*master*"
        "tag:guid": "{{ guid }}"
    register: __control_plane

  - name: Add Control plane Instances to Inventory
    add_host:
      name: "{{ item.private_dns_name }}"
      groups: control_plane
    loop: "{{ __control_plane.instances }}"
    when: __control_plane.instances[0] is defined

  - name: Get Cluster VPC ID
    set_fact:
      cluster_vpc: "{{ __control_plane.instances[0].vpc_id }}"
      # chop off the last character
      cluster_region: "{{ __control_plane.instances[0].placement.availability_zone| regex_replace('.$') }}"
    when: __control_plane.instances[0] is defined

  - name: get vpc_subnet_id
    ec2_vpc_subnet_info:
      region: "{{ aws_region }}"
      filters:
        vpc-id: "{{ cluster_vpc }}"
        # public
        cidr-block: "10.0.0.0/*"
        # private
        # cidr-block: "192.168.0.0/*"
    register: ec2_vpc_subnet_ids

  - name: short subnet cidr
    set_fact:
      ocp_network_subnet_cidr: "{{ ec2_vpc_subnet_ids.subnets[0].cidr_block }}"

  - name: Create IAM role based on trust policy
    register: __rhel_worker_access_role
    iam_role:
      name: rhel-worker-access-role
      assume_role_policy_document: "{{ lookup('file','./files/ec2-role-trust-policy.json') }}"
      create_instance_profile: true
      description: IAM Role for RHEL Worker trust policy
      state: present

  - name: Create IAM access policy
    iam_policy:
      state: present
      iam_type: role
      iam_name: rhel-worker-access-role
      policy_name: rhel-worker-permissions-policy
      policy_json: "{{ lookup('file', './files/ec2-role-access-policy.json') }}"

  - name: Create RHEL Worker security group
    register: sg_RHEL_worker
    ec2_group:
      region: "{{ aws_region }}"
      name: "RHEL_worker-{{ cluster_name }}"
      description: RHEL_worker security group
      tags:
        type: RHEL_worker
      vpc_id: "{{ cluster_vpc }}"
      rules:
      - proto: udp
        ports: 632
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 22
        rule_desc: "SSH"
        cidr_ip: 0.0.0.0/0
      - proto: udp
        ports: 5353
        rule_desc: "mDNS"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 80
        rule_desc: "Ingress HTTP"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 443
        rule_desc: "Ingress HTTPS"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 1936
        rule_desc: "router stats"
        cidr_ip: 0.0.0.0/0
      - proto: udp
        ports: 4789
        rule_desc: "VXLAN"
        cidr_ip: 0.0.0.0/0
      - proto: udp
        ports: 6081
        rule_desc: "Geneve"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 9000-9999
        rule_desc: "Worker ingress internal (tcp)"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 10250
        rule_desc: "master ingress kubelet secure"
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        ports: 30000-32767
        rule_desc: "worker ingress services (tcp)"
        cidr_ip: 0.0.0.0/0
      - proto: udp
        ports: 30000-32767
        rule_desc: "worker ingress services (udp)"
        cidr_ip: 0.0.0.0/0

  - name: set a fact for the instance tags
    set_fact:
      instance_tags: >-
        {
          "type": "RHEL_worker",
          "guid": "{{ guid }}",
          "cluster_name": "{{ cluster_name }}",
          "kubernetes.io/cluster/{{ cluster_name }}": "{{ ocp4_workload_rhel_worker_ownership }}"
        }

  - name: Launch EC2 RHEL_worker instance
    ec2:
      region: "{{ aws_region }}"
      key_name: "opentlc_admin_backdoor"
      vpc_subnet_id: "{{ ec2_vpc_subnet_ids.subnets[0].subnet_id }}"
      instance_type: m5.4xlarge
      group_id: "{{ sg_RHEL_worker.group_id }}"
      image: "{{ __rhel_ami.images[0].image_id }}"
      assign_public_ip: true
      exact_count: "{{ ocp4_workload_rhel_worker_exact_count }}"
      count_tag: "{{ instance_tags }}"
      # instance_tags: "{{ instance_tags }}"
      instance_tags: "{{ instance_tags | combine(hostvars.localhost.cloud_tags_final) }}"
      instance_profile_name: "rhel-worker-access-role"
      wait: true
    register: __new_RHEL

  - name: add new ec2 hosts to inventory
    add_host:
      hostname: "{{ item.public_dns_name }}"
      key_name: "{{ item.key_name }}"
      ansible_user: "ec2-user"
      remote_user: "ec2-user"
      groups: [RHEL_Workers]
      dns_name: "{{ item.dns_name }}"
      private_dns_name: "{{ item.private_dns_name }}"
      private_ip: "{{ item.private_ip }}"
      public_dns_name: "{{ item.public_dns_name }}"
      public_ip: "{{ item.public_ip }}"
      region: "{{ item.region }}"
      tags: "{{ item.tags }}"
    loop: "{{ __new_RHEL.instances }}"

  - name: Wait for SSH to come up
    delegate_to: "{{ item.public_dns_name }}"
    wait_for_connection:
      delay: 60
      timeout: 320
    loop: "{{ __new_RHEL.instances }}"

    # new instances create done

- name: Print inventory information
  debug:
    # verbosity: 3
    msg: "{{ hostvars | to_nice_yaml }}"

- name: satellite - Run setup if gather_facts hasn't been run
  setup:
    gather_subset: min
  when: ansible_date_time is not defined

- name: debug
  loop: "{{ groups['RHEL_Workers'] }}"
  loop_control:
    loop_var: __rw_debug
  debug:
    msg: "RHEL_Workers: {{ __rw_debug }}"

- name: satellite - Setup RHEL Repositories on the RHEL Worker
  loop: "{{ groups['RHEL_Workers'] }}"
  loop_control:
    loop_var: __rw_sat
  include_role:
    name: set-repositories
    apply:
      delegate_to: "{{ __rw_sat }}"
      delegate_facts: true
      become: true
      vars:
        set_repositories_subscription_hostname: "{{ __rw_sat }}-{{ ansible_date_time.iso8601_basic | lower }}"

- name: Add tmux to RHEL_workers
  loop: "{{ groups['RHEL_Workers'] }}"
  loop_control:
    loop_var: __rw_rpm
  delegate_to: "{{ __rw_rpm }}"
  delegate_facts: true
  become: true
  package:
    state: present
    name: tmux

# prep the bastion with openshift-ansible
- name: set openshift-ansible path
  set_fact:
    openshift_ansible_path: "/home/{{ ansible_user }}/openshift-ansible/"

- name: clone the openshift-ansible repo
  git:
    repo: 'https://github.com/openshift/openshift-ansible'
    version: release-4.9
    dest: "{{ openshift_ansible_path }}"
    depth: 1

- name: put the RHEL workers names in a file
  loop: "{{ groups['RHEL_Workers'] }}"
  loop_control:
    loop_var: __rw_file
  lineinfile:
    path: ~/RHEL_workers.txt
    line: "{{ __rw_file }}"
    create: true

# Leave this as the last task in the playbook.
- name: workload tasks complete
  debug:
    msg: "Workload Tasks completed successfully."
  when: not silent | bool
