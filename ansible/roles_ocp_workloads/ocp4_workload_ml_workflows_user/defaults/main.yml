---
become_override: false
ocp_username: opentlc-mgr
silent: false

user_count: 10
user_count_start: 1

instructor_mode: false

use_openshift_build: false
use_openshift_pipelines: true

pipeline_builder_image: quay.io/willbenton/simple-model-s2i:cached-pipeline-s2i
pipeline_notebook_repository: https://github.com/willb/fraud-notebooks
pipeline_notebook_list: 02-feature-engineering.ipynb,03-model-logistic-regression.ipynb

ocs_operator_workload_destroy: "{{ False if (ACTION=='create' or ACTION=='provision') else True }}"

odh_project: open-data-hub
ocp4_workload_ml_workflows_user_project: s2s-{{ guid }}-project
workload_destroy: false

och_expected_crds:
- opendatahubs.opendatahub.io

knative_expected_crds:
- knativeservings.operator.knative.dev

install_ansible_openshift: true

install_storage: false
install_open_data_hub: true
install_postgres: false
install_codeready: false
install_knative: true
install_pipelines: true


# Command separated string list each git repo url to preload on the notebook pod when it spawns
workshop_preload_repos: "https://github.com/willb/fraud-notebooks"


jupyterhub_image_registry: 'quay.io'
jupyterhub_image_repository: 'odh-jupyterhub'

# Custom notebook image source that will be used for the workshop
workshop_jupyter_notebook_imagestream_image: "quay.io/willbenton/jh-ml-workflows-notebook:summit2020"
# Imagestream name for the custom workshop image
workshop_jupyter_notebook_imagestream_name: "ml-workflows-notebook"
# Imagestream tag for the custom workshop image
workshop_jupyter_notebook_imagestream_tag: "latest"

rgw_endpoint_url: "https://s3.foo.com:8000"

# Image to use for the Spark Cluster
spark_node_image: "quay.io/llasmith/openshift-spark:spark-2.3.2_hadoop-2.8.5"
# Number of Spark master nodes
spark_master_count: 1
# Number of Spark worker nodes
spark_worker_count: 2
# Amount of memory to allocate to each Spark node. This amount will be used for master AND worker nodes
spark_node_memory: "4Gi"
# Amount of cpu to allocate to each Spark node. This amount will be used for master AND worker nodes
spark_node_cpu: 1

# Path to append to env var PYTHONPATH for pyspark module
# yamllint disable-line rule:line-length
spark_pythonpath: "/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip"
# PySpark submit args to be set as the env var SPARK_SUBMIT_ARGS
spark_submit_args: >-
  --conf spark.cores.max=1
  --conf spark.executor.instances=1
  --conf spark.executor.memory=4G
  --conf spark.executor.cores=1
  --conf spark.driver.memory=2G
  --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5
  pyspark-shell

# Amount of memory for the JupyterHub server
jupyterhub_memory: "1Gi"
# Amount of memory for the spawned Jupyter Notebook pods
jupyter_notebook_memory: "4Gi"

## Catalogue snap shot:

#This variable has no effect when using a catalog snapshot (always true)
ocp4_workload_odh_automatic_install_plan_approval: true

ocp4_workload_odh_use_catalog_snapshot: true
ocp4_workload_odh_snapshot_name: community-operators-snapshot-odh
ocp4_workload_odh_catalog_snapshot_image: quay.io/gpte-devops-automation/olm_snapshot_community_catalog
ocp4_workload_odh_snapshot_image_tag: "v4.4_2020_10_01"
ocp4_workload_odh_starting_csv: ""
