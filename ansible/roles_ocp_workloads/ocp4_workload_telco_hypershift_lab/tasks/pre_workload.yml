---
# Implement your Pre Workload deployment tasks here

- name: Fail if pull-secret var is empty
  ansible.builtin.assert:
    that:
      - ocp4_pull_secret is defined
      - ocp4_pull_secret | default("") | length > 0
    fail_msg: ocp4_pull_secret variable must be defined
    success_msg: ocp4_pull_secret variable is defined

- name: Ensure ansible module requirements are installed
  ansible.builtin.pip:
    name: kubernetes, passlib

- name: Find an extra disk for libvirt folder
  set_fact:
    _disks: "{{ _disks|default([]) + [ item ] }}"
    _diskname: "{{ '/dev/' + item.key }}"
  when:
    - not item.value.partitions
    - item.key is search ("nvme*")
    - min_value|int <= item.value.sectors|int * item.value.sectorsize|int
    - extra_disk_libvirt_images is true
  vars:
    min_value: '{{ 500 * 1024 * 1024 * 1024  }}' #500GB
  with_dict: "{{ ansible_devices }}"

- name: The device selected to store libvirt files
  debug:
    msg: "{{ item }}"
  with_dict: "{{ _disks }}"
  when:
    - _diskname is defined

- name: Create a xfs filesystem on {{ _diskname }}
  community.general.filesystem:
    fstype: "xfs"
    dev: "{{ _diskname }}"
    force: true
    state: present
    resizefs: true
  when:
    - _diskname is defined

- name: Mount up device in /var/lib/libvirt
  ansible.posix.mount:
    path: /var/lib/libvirt
    src: "{{ _diskname }}"
    fstype: xfs
    state: mounted
  when:
    - _diskname is defined

- name: Ensure libvirt file applies the proper SELinux context
  ansible.builtin.shell:
    cmd: restorecon /var/lib/libvirt

- name: Ensure epel repo is enabled
  ansible.builtin.dnf:
    name: "{{ epel_release }}"
    state: present
    disable_gpg_check: true

# group all dnf installs in the same task to save time
- name: Ensure lab dependencies are installed
  ansible.builtin.dnf:
    name:
      - libvirt
      - libvirt-daemon-driver-qemu
      - qemu-kvm
      - bash-completion
      - vim
      - jq
      - tar
      - python3-cherrypy
      - git
      - dnsmasq
      - policycoreutils-python-utils
      - podman
      - httpd-tools
      - haproxy
      - chrony
    state: present

- name: Ensure kcli rpm is downloaded
  ansible.builtin.get_url:
    url: "{{ kcli_rpm_url }}"
    dest: /var/tmp/kcli.rpm

- name: Ensure kcli rpm is installed
  ansible.builtin.dnf:
    name: /var/tmp/kcli.rpm
    state: present
    disable_gpg_check: true

- name: Ensure libvirtd service is enabled and running
  ansible.builtin.systemd:
    state: restarted
    daemon_reload: true
    enabled: true
    name: libvirtd

- name: Ensure default image pool is present
  ansible.builtin.shell:
    cmd: kcli create pool -p /var/lib/libvirt/images default

- name: Ensure default network is present
  ansible.builtin.shell:
    cmd: kcli create network -c 192.168.122.0/24 default

- name: Ensure lab network is present
  ansible.builtin.shell:
    # yamllint disable rule:line-length
    cmd: "kcli create network -c {{ lab_network_cidr }} -P dhcp=false -P dns=false --domain {{ lab_network_domain }} hypershiftlab"
    # yamllint enable rule:line-length

- name: Ensure oc tooling is present
  ansible.builtin.shell:
    cmd: kcli download oc -P version=stable -P tag='{{ ocp4_major_release }}'

- name: Ensure oc is in the PATH
  ansible.builtin.copy:
    src: "oc"
    dest: "/usr/bin/oc"
    mode: "0755"
    remote_src: true

- name: Ensure DNSMasq folder exists
  ansible.builtin.file:
    path: /opt/dnsmasq/include.d/
    state: directory
    mode: "0755"

- name: Ensure DNSMasq config files exist
  ansible.builtin.get_url:
    url: "{{ item.url }}"
    dest: "{{ item.destination }}"
    mode: "{{ item.mode }}"
  # yamllint disable rule:line-length
  with_items:
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/dnsmasq.conf", destination: "/opt/dnsmasq/dnsmasq.conf", mode: "0644"}
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/upstream-resolv.conf", destination: "/opt/dnsmasq/upstream-resolv.conf", mode: "0644"}
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/management.ipv4", destination: "/opt/dnsmasq/include.d/management.ipv4", mode: "0644"}
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/hosted.ipv4", destination: "/opt/dnsmasq/include.d/hosted.ipv4", mode: "0644"}
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/infrastructure-host.ipv4", destination: "/opt/dnsmasq/include.d/infrastructure-host.ipv4", mode: "0644"}
    - {url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/dnsmasq/dnsmasq-virt.service", destination: "/etc/systemd/system/dnsmasq-virt.service", mode: "0644"}
  # yamllint enable rule:line-length

- name: Ensure DNS Upstream server is configured
  ansible.builtin.replace:
    path: /opt/dnsmasq/upstream-resolv.conf
    regexp: 'UPSTREAM_DNS'
    replace: "{{ upstream_dns }}"

- name: Ensure DNSMasq lease file exists
  ansible.builtin.file:
    path: /opt/dnsmasq/hosts.leases
    state: touch
    mode: "0644"

- name: Ensure DNSMasq lease file has the proper SELinux context
  community.general.sefcontext:
    target: "/opt/dnsmasq/hosts.leases"
    setype: dnsmasq_lease_t
    state: present

- name: Ensure DNSMasq file applies the proper SELinux context
  ansible.builtin.shell:
    cmd: restorecon /opt/dnsmasq/hosts.leases

- name: Ensure DNSMasq service is enabled and running
  ansible.builtin.systemd:
    state: restarted
    daemon_reload: true
    enabled: true
    name: dnsmasq-virt

- name: Ensure dispatcher script exists
  ansible.builtin.get_url:
    url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/hypervisor/forcedns"
    dest: "/etc/NetworkManager/dispatcher.d/forcedns"
    mode: "0755"

- name: Ensure NetworkManager is restarted
  ansible.builtin.systemd:
    state: restarted
    name: NetworkManager

- name: Ensure dispatcher script has run
  ansible.builtin.shell:
    cmd: /etc/NetworkManager/dispatcher.d/forcedns

- name: Ensure ksushy is installed
  ansible.builtin.shell:
    cmd: kcli create sushy-service --ssl --port 9000 --bootonce

- name: Ensure lab VMs exist
  ansible.builtin.shell:
    # yamllint disable rule:line-length
    cmd: "kcli create vm -P start=False -P uefi_legacy=true -P plan=lab -P memory={{ item.memory }} -P numcpus={{ item.cpus }} -P disks=[{{ item.disk }},{{ item.disk }}] -P nets=['{\"name\": \"hypershiftlab\", \"mac\": \"{{ item.mac }}\"}'] -P uuid={{ item.uuid }} -P name={{ item.name }}"
    # yamllint enable rule:line-length
  register: result
  failed_when: result.rc != 0 and "not created because VM" not in result.stderr
  # yamllint disable rule:line-length
  with_items:
    - {name: "hosted-worker0", cpus: "{{ lab_hosted_vm_cpus }}", disk: "{{ lab_hosted_vm_disk }}", memory: "{{ lab_hosted_vm_memory }}", mac: "aa:aa:aa:aa:02:01", uuid: "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201"}
    - {name: "hosted-worker1", cpus: "{{ lab_hosted_vm_cpus }}", disk: "{{ lab_hosted_vm_disk }}", memory: "{{ lab_hosted_vm_memory }}", mac: "aa:aa:aa:aa:02:02", uuid: "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0202"}
    - {name: "hosted-worker2", cpus: "{{ lab_hosted_vm_cpus }}", disk: "{{ lab_hosted_vm_disk }}", memory: "{{ lab_hosted_vm_memory }}", mac: "aa:aa:aa:aa:02:03", uuid: "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0203"}
  # yamllint enable rule:line-length

- name: Ensure install folder exist
  ansible.builtin.file:
    path: /root/mgmt-cluster-install/
    state: directory
    mode: "0755"

- name: Ensure pull secret is copied to the bastion host
  ansible.builtin.copy:
    content: "{{ ocp4_pull_secret }}"
    dest: "/root/mgmt-cluster-install/openshift_pull.json"
    mode: '0644'

- name: Ensure plan file exists
  ansible.builtin.get_url:
  # yamllint disable rule:line-length
    url: "https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{{ lab_version }}/lab-materials/lab-env-data/management-cluster/management.yml"
  # yamllint enable rule:line-length
    dest: "/root/mgmt-cluster-install/management.yml"
    mode: "0644"

- name: Setup admin password
  set_fact:
    strong_admin_password: "{{ lookup('password', '/dev/null length=16 chars=ascii_letters,digits') }}"
    strong_dev_password: "{{ lookup('password', '/dev/null length=16 chars=ascii_letters,digits') }}"

- name: Set password to hub admin user
  ansible.builtin.replace:
    path: "/root/mgmt-cluster-install/management.yml"
    regexp: '{{ item.regexp }}'
    replace: "'{{ item.password }}'"
  with_items:
    - {regexp: "CHANGE_ADMIN_PWD", password: "{{ strong_admin_password }}"}
    - {regexp: "CHANGE_DEV_PWD", password: "{{ strong_dev_password }}"}

- name: Write password to a file
  ansible.builtin.copy:
    content: "{{ item.password }}"
    dest: "{{ item.dest }}"
  with_items:
    - {dest: "/root/cred-admin.txt", password: "{{ strong_admin_password }}"}
    - {dest: "/root/cred-dev.txt", password: "{{ strong_dev_password }}"}

- name: Ensure .ssh directory exists
  ansible.builtin.file:
    path: /root/.ssh
    state: directory
    mode: '0700'

- name: Ensure ssh-key exists
  community.crypto.openssh_keypair:
    path: /root/.ssh/id_rsa


- name: Ensure ksushy is listening for redfish connections
  ansible.builtin.uri:
    url: https://infra.{{ lab_network_domain }}:9000/redfish/v1/Systems/local/hosted-worker0
    force_basic_auth: true
    user: admin
    password: admin
    method: GET
    status_code: 200
    validate_certs: false

# Leave this as the last task in the playbook.
- name: pre_workload tasks complete
  debug:
    msg: "Pre-Workload tasks completed successfully."
  when: not silent|bool
