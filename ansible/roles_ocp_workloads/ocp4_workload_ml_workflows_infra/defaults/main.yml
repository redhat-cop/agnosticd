ocs_expected_crds:
- backingstores.noobaa.io
- bucketclasses.noobaa.io
- noobaas.noobaa.io
- objectbucketclaims.objectbucket.io
ocs_channel: stable-4.2
ocs_install_mcg: true
ocs_mcg_core_cpu: 0.5
ocs_mcg_core_mem: 500Mi
ocs_mcg_db_cpu: 0.5
ocs_mcg_db_mem: 1Gi
ocs_ceph_mds_cpu: 0.5
ocs_ceph_mds_mem: 1Gi
ocs_ceph_mon_cpu: 0.5
ocs_ceph_mon_mem: 1Gi
ocs_ceph_mgr_cpu: 0.5
ocs_ceph_mgr_mem: 1Gi
ocs_ceph_osd_cpu: 0.5
ocs_ceph_osd_mem: 1Gi
ocs_mcg_pv_pool: true
ocs_mcg_pv_pool_bucket_name: mcg
ocs_mcg_pv_pool_pv_size: 50Gi
ocs_mcg_pv_pool_pv_quantity: 3
ocs_mcg_pv_pool_pv_storageclass: gp2
ocs_namespace: openshift-storage
ocs_release: v4.2.2
ocs_operator_workload_destroy: "{{ False if (ACTION=='create' or ACTION=='provision') else True }}"
silent: false

# Command separated string list each git repo url to preload on the notebook pod when it spawns
workshop_preload_repos: "https://github.com/willb/fraud-notebooks"

och_project: open-data-hub
user_project: user
workload_destroy: false

och_expected_crds:
- opendatahubs.opendatahub.io

knative_expected_crds:
- knativeservings.operator.knative.dev

install_ansible_openshift: true

install_storage: false
install_open_data_hub: true
install_postgres: false
install_codeready: false
install_knative: true
install_pipelines: true

# The first user number to start with when creating projects
user_count_start: 1
# The total number of users when creating projects
user_count: 10

# Amount of memory for the JupyterHub server
jupyterhub_memory: "1Gi"
# Amount of memory for the spawned Jupyter Notebook pods
jupyter_notebook_memory: "4Gi"

# Image to use for the Spark Cluster
spark_node_image: "quay.io/llasmith/openshift-spark:spark-2.3.2_hadoop-2.8.5"
# Number of Spark master nodes
spark_master_count: 1
# Number of Spark worker nodes
spark_worker_count: 2
# Amount of memory to allocate to each Spark node. This amount will be used for master AND worker nodes
spark_node_memory: "4Gi"
# Amount of cpu to allocate to each Spark node. This amount will be used for master AND worker nodes
spark_node_cpu: 1

# Path to append to env var PYTHONPATH for pyspark module
# yamllint disable-line rule:line-length
spark_pythonpath: "/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip"
# PySpark submit args to be set as the env var SPARK_SUBMIT_ARGS
# yamllint disable-line rule:line-length
spark_submit_args: "--conf spark.cores.max=1 --conf spark.executor.instances=1 --conf spark.executor.memory=4G --conf spark.executor.cores=1 --conf spark.driver.memory=2G --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5 pyspark-shell"

jupyterhub_image_registry: 'quay.io'
jupyterhub_image_repository: 'odh-jupyterhub'

# Custom notebook image source that will be used for the workshop
workshop_jupyter_notebook_imagestream_image: "quay.io/willbenton/jh-ml-workflows-notebook:summit2020"
# Imagestream name for the custom workshop image
workshop_jupyter_notebook_imagestream_name: "ml-workflows-notebook"
# Imagestream tag for the custom workshop image
workshop_jupyter_notebook_imagestream_tag: "latest"

rgw_endpoint_url: "https://s3.foo.com:8000"
