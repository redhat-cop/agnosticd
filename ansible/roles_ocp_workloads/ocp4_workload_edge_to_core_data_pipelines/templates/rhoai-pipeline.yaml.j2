{% for __user in users %}
{% for __deployment in deployments %}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pipeline-pvc
  namespace: {{ __deployment.namespace | replace("user", __user) }}
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: train-model
  namespace: {{ __deployment.namespace | replace("user", __user) }}
spec:
    params:
    - name: aws_access_key
      default: {{ __deployment.access_key | default(__user) }}
    - name: aws_secret_key
      default: {{ __deployment.secret_key | default('secret') }}
    - name: s3bucket_data
      default: {{ __user }}-data
    - name: s3bucket_models
      default: {{ __user }}-models
    - name: s3bucket_ready
      default: {{ __user }}-ready
    - name: s3endpoint
      default: {{ __deployment.s3_endpoint | default('http://minio-service:9000') | replace("user", __user) }}
    - name: working_dir
      default: /data/edge1/
    tasks:
    - name: run-a-file
      params:
      - name: aws_access_key
        value: $(params.aws_access_key)
      - name: aws_secret_key
        value: $(params.aws_secret_key)
      - name: s3bucket_data
        value: $(params.s3bucket_data)
      - name: s3endpoint
        value: $(params.s3endpoint)
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_data="$1"
            working_dir="$2"
            aws_access_key="$3"
            aws_secret_key="$4"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint '{{ __deployment.s3_endpoint | default('http://minio-service:9000') | replace("user", __user) }}' --cos-bucket 'workbench' --cos-directory 'retrain-0325111328' --cos-dependencies-archive 'step-01-6764011e-55f5-4d4d-a8c2-252b1de09bc8.tar.gz' --file 'step-01.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_data=$s3bucket_data;working_dir=$working_dir;aws_access_key=$aws_access_key;aws_secret_key=$aws_secret_key' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_data)
          - $(inputs.params.working_dir)
          - $(inputs.params.aws_access_key)
          - $(inputs.params.aws_secret_key)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            value: {{ __deployment.access_key | default(__user) }}
          - name: AWS_SECRET_ACCESS_KEY
            value: {{ __deployment.secret_key | default('secret') }}
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: aws_access_key
        - name: aws_secret_key
        - name: s3bucket_data
        - name: s3endpoint
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: load_data
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-01.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: load_data
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=170a691197740fb41dc687dab1be34b8cab469e8fafd24494d61b484533b9b0f"}'
    - name: run-a-file-2
      params:
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            working_dir="$0"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint '{{ __deployment.s3_endpoint | default('http://minio-service:9000') | replace("user", __user) }}' --cos-bucket 'workbench' --cos-directory 'retrain-0325111328' --cos-dependencies-archive 'step-02-8af3247b-bc48-42f1-9f25-95e1de81169a.tar.gz' --file 'step-02.ipynb' --pipeline-parameters 'working_dir=$working_dir' --parameter-pass-method 'env' "
          - $(inputs.params.working_dir)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            value: {{ __deployment.access_key | default(__user) }}
          - name: AWS_SECRET_ACCESS_KEY
            value: {{ __deployment.secret_key | default('secret') }}
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: create_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-02.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: create_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=ee71534e62e0af8d27defee34e508b2f3ccbb855f8feebab8dc9f02b06c1571d"}'
      runAfter:
      - run-a-file
    - name: run-a-file-3
      params:
      - name: aws_access_key
        value: $(params.aws_access_key)
      - name: aws_secret_key
        value: $(params.aws_secret_key)
      - name: s3bucket_models
        value: $(params.s3bucket_models)
      - name: s3bucket_ready
        value: $(params.s3bucket_ready)
      - name: s3endpoint
        value: $(params.s3endpoint)
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_models="$1"
            s3bucket_ready="$2"
            working_dir="$3"
            aws_access_key="$4"
            aws_secret_key="$5"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint '{{ __deployment.s3_endpoint | default('http://minio-service:9000') | replace("user", __user) }}' --cos-bucket 'workbench' --cos-directory 'retrain-0325111328' --cos-dependencies-archive 'step-03-9860d935-5991-4391-a6dd-2e0af6f912a3.tar.gz' --file 'step-03.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_models=$s3bucket_models;s3bucket_ready=$s3bucket_ready;working_dir=$working_dir;aws_access_key=$aws_access_key;aws_secret_key=$aws_secret_key' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_models)
          - $(inputs.params.s3bucket_ready)
          - $(inputs.params.working_dir)
          - $(inputs.params.aws_access_key)
          - $(inputs.params.aws_secret_key)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            value: {{ __deployment.access_key | default(__user) }}
          - name: AWS_SECRET_ACCESS_KEY
            value: {{ __deployment.secret_key | default('secret') }}
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: aws_access_key
        - name: aws_secret_key
        - name: s3bucket_models
        - name: s3bucket_ready
        - name: s3endpoint
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: push_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-03.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: push_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=650cac7105cc33e7efc4c3a378fdf265ae702f84b25f727a816986154eb1e78b"}'
      runAfter:
      - run-a-file-2

---
apiVersion: triggers.tekton.dev/v1alpha1
kind: TriggerTemplate
metadata:
  name: train-model-template
  namespace: {{ __deployment.namespace | replace("user", __user) }}
spec:
  params:
  - name: id-edge
    description: Identifier of target edge enviroment.
  resourcetemplates:
    - apiVersion: tekton.dev/v1beta1
      kind: PipelineRun
      metadata:
        generateName: train-model-run-
      spec:
        pipelineRef:
          name: train-model
        params:
        - name: s3bucket_data
          value: $(tt.params.id-edge)-data
        - name: s3bucket_models
          value: $(tt.params.id-edge)-models
        - name: s3bucket_ready
          value: $(tt.params.id-edge)-ready
        - name: working_dir
          value: /data/$(tt.params.id-edge)/
        - name: aws_access_key
          value: {{ __deployment.access_key | default(__user) }}
        - name: aws_secret_key
          value: {{ __deployment.secret_key | default('secret') }}
---
apiVersion: triggers.tekton.dev/v1alpha1
kind: TriggerBinding
metadata:
  name: train-model-binding
  namespace: {{ __deployment.namespace | replace("user", __user) }}
spec:
  params:
    - name: id-edge
      value: $(body.id-edge)
---
apiVersion: triggers.tekton.dev/v1alpha1
kind: EventListener
metadata:
  name: train-model-listener
  namespace: {{ __deployment.namespace | replace("user", __user) }}
spec:
  serviceAccountName: pipeline
  triggers:
    - name: trigger-listener
      bindings:
      - ref: train-model-binding
      template:
        ref: train-model-template
{% endfor %}
{% endfor %}