= ocp4_workload_example_shared_cluster - Example Workload Role deployed to Shared Clusters

== Preparation to Use This Role

=== Get Access to Shared Cluster and Create Quotas

This example role assumes you'll be using the Shared Cluster provided by GPTE.
There should be a catalog item in CloudForms to give you access and get started developing.

== Description

This role teaches the use of agnosticD ocp4_workload roles on pre-existing OpenShift clusters where admin access to the cluster is not available.
When working with this role from the command line, the user must have "self-provisioner" roles and quotas already setup by another process, usually labs.opentlc.com or rhpds.redhat.com.
When working from AgnosticV, this role will likely be executed by `system:admin`.
This role deploys a simple application using our best-practices for deploying OpenShift resources.  
It makes as much use of the Ansible module `k8s` as possible.
This role demonstrates writing code to wait for OpenShift tasks to complete before continuing.
This role demonstrates use the 'user.info' debug statements for sending messages to the user.
When removed by the deployer (CloudForms or command line), this role deletes resources it created.
****
For more information on dedicated clusters and setting up self-provisioner capable users and quotas on your own dedicated clusters, see "ocp4_workload_shared_cluster_access"
****

== Role overview

* This is a working simple role that can be used to develop new OpenShift 4 workload roles on shared clusters. It consists of the following playbooks:

=== Playbooks

** Playbook: link:./tasks/pre_workload.yml[pre_workload.yml] - Sets up an environment for the workload deployment
*** Debug task will print out: `pre_workload Tasks completed successfully.`

** Playbook: link:./tasks/workload.yml[workload.yml] - Used to deploy the actual workload, i.e, 3scale, Mobile, some Demo or OpenShift customization
*** Read this role for our recommended best practices for working with ansible and OpenShift
*** This role creates an OpenShift project, deploys a simple application, and notifies on success or failure.
*** Debug task will print out: `workload Tasks completed successfully.`

** Playbook: link:./tasks/post_workload.yml[post_workload.yml] - Used to configure the workload after deployment
*** This role doesn't do anything
*** Debug task will print out: `post_workload Tasks completed successfully.`

** Playbook: link:./tasks/remove_workload.yml[remove_workload.yml] - Used to delete the workload
*** This role deletes the OpenShift projected created for the user.
*** Debug task will print out: `remove_workload Tasks completed successfully.`

=== The defaults variable file

* This file link:./defaults/main.yml[./defaults/main.yml] contains all the variables you need to define to control the deployment of your workload.
* The variable *ocp_username* is mandatory to assign the workload to the correct OpenShift user.
* A variable *silent=True* can be passed to suppress debug messages.
* You can override any of these default values by adding `-e "variable_name=variable_value"` to the command line
* Your deployer will override any of these variables (usually CloudForms)
* Note that this role uses long-name scoped workload parameters. Example: `ocp4_workload_example_shared_cluster_project_name: my-project-name`, not just `project_name`

=== Variable Naming

You *must* use long-name scope parameters for your workload to avoid variable clashing.
Ansible lacks robust variable scoping.
For example, parameters named `ocp4_workload_example_*` would be recognized as unique to this workload


You must use long-name scope parameters for your workload.
Ansible lacks robust variable scoping.
For parameters passed into your workload role, prepend the role name.
For parameters shared across workloads, have a catch-all name that applies to your goal.
For example, parameters named `ocp4_workload_example_shared_cluster_*` would apply to workloads `ocp4-workload-example_shared_cluster` itself *and* `ocp4_workload_example_shared_cluster_minio`.

=== Provide info to users

To provide information to users, use the agnosticd_user_info module built into AgnosticD.  You'll also see this information in the agnosticd workload logs.

.Using the `agnosticd_user_info` module
----
  - name: Report URL for web console
    agnosticd_user_info:
      msg: "OpenShift Web Console: {{ _openshift_console_url }}
----

In RHPDS and OpenTLC, CloudForms will pick up this information and email it to your user.

More info about the `agnosticd_user_info` module here: link:../../../docs/User_Info.adoc

== Deploy a Workload from the command line with the `ocp-workload` playbook [Mostly for testing]

This example workload is geared for use with the existing GPTE Shared OCP clusters, or your own clusters that you maintain for various purposes.
Your workload might be an OpenShift demo on RHPDS's shared clustesr, or one of many workloads all used for a demo or workshop.

TODO: explain getting share cluster access via OpenTLC/RHDPS, etc.
TODO: explain deploying from CloudForms - `workload_shared_deployment: true`

. If your workload uses parameters create a `<role name>_vars.yaml` input file.
+
.ocp4_workload_example_shared_cluster_vars.yaml
[source,yaml]
----
# You can set any variable
silent: true

# Set a variable scoped to the role.  In this case, a variable for a project name to be created.
ocp4_workload_example_shared_cluster_project_name: "sample-application-{{ ocp_username }}"
----

. Set up Environment Variables for the bastion you want to run this role on.
+
[source,bash]
----
TARGET_HOST="bastion.dev.openshift.opentlc.com"
OCP_USERNAME="wkulhane-redhat.com"
ANSIBLE_USER="ec2-user" # Will become OpenTLC username
WORKLOAD="ocp4_workload_example_shared_cluster"
GUID="1001"
----

. Finally run the workload passing the input files as parameters:
+
[source,sh]
----
# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
    -e"ansible_ssh_private_key_file=~/.ssh/keytoyourhost.pem" \
    -e"ansible_user=${ANSIBLE_USER}" \
    -e"ocp_username=${OCP_USERNAME}" \
    -e"ocp_workload=${WORKLOAD}" \
    -e"guid=${GUID}" \
    -e"ACTION=create" \
    -e @./ocp4_workload_example_shared_cluster_vars.yaml \
    -e @./ocp4_workload_example_shared_cluster_secrets.yaml
----
+

=== To Delete a Workload from the CLI

----
TARGET_HOST="bastion.dev.openshift.opentlc.com"
OCP_USERNAME="wkulhane-redhat.com"
ANSIBLE_USER="ec2-user" # Will become OpenTLC username
WORKLOAD="ocp_workload_example_shared_cluster"
GUID="1001"

# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
    -e"ansible_ssh_private_key_file=~/.ssh/keytoyourhost.pem" \
    -e"ansible_user=ec2-user" \
    -e"ocp_username=${OCP_USERNAME}" \
    -e"ocp_workload=${WORKLOAD}" \
    -e"guid=${GUID}" \
    -e"ACTION=remove" \
    -e @./ocp_workload_example_shared_cluster_vars.yaml \
    -e @./ocp_workload_example_shared_clsuter_secrets.yaml
----


== Deploying a Workload with AgnosticV from the Command Line

When creating a configuration in AgnosticV that includes the deployment of the workload you can specify the variables straight in the AgnosticV config.
AgnosticV configs are usually created by combining a `common.yaml` file with either `dev.yaml`, `test.yaml` or `prod.yaml`.
You can specify different variables in each of these files.
For example you could have common values defined in the `common.yaml` file and then specific values overriding the common ones for development or production environments in `dev.yaml` or `prod.yaml`.

AgnosticV merges the definition files starting with `common.yaml` and then adding/overwriting what comes from either `dev.yaml` or `prod.yaml`.

Example of a simple AgnosticV config:

.common.yaml
[source,yaml]
----
# --- Example Shared Cluster Workload Deployment for RPDS
# --- System: RHPDS
# --- Catalog: OpenShift Demos
# --- Catalog Item: Quay 3 on OpenShift 4

# --- Platform
platform: rhpds

# --- Cloud Provider
cloud_provider: none

# --- Config
env_type: ocp-workload
ocp_workload: ocp4_workload_example_shared_cluster
# If your workload requires sudo, additional privileges are required.  
# For now, workload must be run as ec2-user (or cloud-user on OpenStack)
ansible_user: ec2-user
ansible_ssh_private_key_file: /home/opentlc-mgr/.ssh/opentlc_admin_backdoor.pem

# --- Ensure the workload prints the correct statements for CloudForms to realize it finished
workload_shared_deployment: true

# --- Workload Configuration
ocp4_workload_example_shared_cluster_project_name: "ocp4-workload-example-{{ ocp_username }}"

# --- AgnosticV Meta variables
agnosticv_meta:
  params_to_variables:
    user: ocp_username
  secrets:
  # A secret file that might hold something your role needs
  - ocp4_workload_example_shared_cluster_secrets.yml
----

.dev.yaml
[source,yaml]
----
purpose: development

# --- Use specific variable values for Development
target_host: bastion.dev4.openshift.opentlc.com

# --- Workload Configuration Overrides
ocp4_workload_example_shared_cluster_project_name:  "ocp4-workload-example-{{ ocp_username }}-dev"
----

.prod.yaml
[source,yaml]
----
---
purpose: production

# --- Use specific variable values for Production
target_host: bastion.rhpds.openshift.opentlc.com

# --- Workload Configuration Overrides
ocp4_workload_example_shared_cluster_project_name:  "ocp4-workload-example-{{ ocp_username }}-prod"

# --- AgnosticV Meta variables
agnosticv_meta:
  agnosticd_git_tag_prefix: ocp4-workload-example-shared-cluster-prod
----

== Further Learning - More Complex Examples

If you want to see more examples of how this works in a real world workload the following workloads already use this approach:

* ocp4_workload_example_dedicated_cluster
* ocp4_workload_authentication
* ocp4_workload_machinesets
* ocp4_workload_logging
* ocp4_workload_quay_operator

////
=== User Authentication on Dedicated Clusters

NOTE: Use only for dedicated cluster.  Do not use on a Shared Cluster.

The `ocp4_workload_authentication` role allows you to setup the authentication system that makes sense for you:
. link:https://github.com/redhat-cop/agnosticd/blob/development/ansible/roles_ocp_workloads/ocp4_workload_authentication/defaults/main.yml

Use the `ocp4_workload_shared_cluster_access` workload to give a user access to the shared cluster.

.Add the following variables, and the workload will give access and create a quota:
----
ocp_workload: ocp4_workload_shared_cluster_access
ocp_username: <your OpenTLC username>
----

.Here are the details of the quota created by the workload:
https://github.com/redhat-cop/agnosticd/blob/development/ansible/roles_ocp_workloads/ocp4_workload_shared_cluster_access/defaults/main.yml

////
