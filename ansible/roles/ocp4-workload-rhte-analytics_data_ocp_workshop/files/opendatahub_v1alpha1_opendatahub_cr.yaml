apiVersion: opendatahub.io/v1alpha1
kind: OpenDataHub
metadata:
  name: example-opendatahub
spec:
  # JupyterHub deployment developed by Graham Dumpleton - https://github.com/aicoe/jupyterhub-ocp-oauth
  aicoe-jupyterhub:
    # Deploy the ODH aicoe-jupyterhub role if True
    odh_deploy: true
    notebook_memory: 2Gi
    deploy_all_notebooks: False
    registry: ''
    repository: ''
    storage_class: ''
    db_memory: 1Gi
    jupyterhub_memory: 1Gi
    notebook_image: 's2i-minimal-notebook:3.6'
    s3_endpoint_url: ''

    # Name of the configmap that will be used when spawning a notebook for the single user
    spark_configmap_template: 'jupyterhub-spark-operator-configmap'
    # PYSPARK args to use in the notebook pod
    # These submit args should be customized for the values passed for spark_memory and spark_cpu. You'll need to account for the available memory on the spark work nodes
    spark_pyspark_submit_args: "--conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell"
    spark_pyspark_driver_python: "jupyter"
    spark_pyspark_driver_python_opts: "notebook"
    spark_home: "/opt/app-root/lib/python3.6/site-packages/pyspark/"
    spark_pythonpath: "$PYTHONPATH:/opt/app-root/lib/python3.6/site-packages/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.8.2.1-src.zip"

    # Number of master and worker nodes for the spark cluster
    spark_worker_nodes: 2
    spark_master_nodes: 1

    # Amount of cpu & memory to allocate to the each node in the cluster.
    # This value will be applied to all worker and master nodes
    spark_memory: 4Gi
    spark_cpu: 3
    # Spark image to use in the cluster
    spark_image: "quay.io/opendatahub/spark-cluster-image:spark22python36"

  # Spark operator developed by radanalyticsio - https://github.com/radanalyticsio/spark-operator
  spark-operator:
    # Deploy the ODH spark-operator role if True
    odh_deploy: true
    master_node_count: 0
    master_memory: 1Gi
    master_cpu: 1
    worker_node_count: 0
    worker_memory: 2Gi
    worker_cpu: 2

  # Seldon Delployment
  seldon:
    odh_deploy: false

  # JupyterHub deployment developed by Graham Dumpleton - https://github.com/jupyter-on-openshift/jupyterhub-quickstart
  jupyter-on-openshift:
    # Deploy the ODH jupyter-on-openshift role if True
    odh_deploy: false
    notebook_memory: 2Gi
    # Add these whitelisted environment variables from JupyterHub to the user's notebook pod
    jupyterhub_config: |
      c.KubeSpawner.env_keep = ['S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY']
    # Environment variables that will be set on the JupyterHub pod
    extra_env_vars:
      S3_ENDPOINT_URL: "http://s3.foo.com:8000"
      S3_ACCESS_KEY: "YOURS3ACCESSKEYHERE"
      S3_SECRET_KEY: "this1is2just3gibberish"

  # Deployment of Prometheus and Grafana for Monitoring of ODH
  monitoring:
    odh_deploy: true

  # Deployment of Two Sigma's BeakerX Jupyter notebook
  beakerx:
    odh_deploy: false
