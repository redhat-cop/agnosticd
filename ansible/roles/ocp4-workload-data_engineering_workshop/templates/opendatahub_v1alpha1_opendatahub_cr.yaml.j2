apiVersion: opendatahub.io/v1alpha1
kind: OpenDataHub
metadata:
  name: example-opendatahub
spec:
  # JupyterHub deployment developed by Graham Dumpleton - https://github.com/aicoe/jupyterhub-ocp-oauth
  aicoe-jupyterhub:
    # Deploy the ODH aicoe-jupyterhub role if True
    odh_deploy: true
    notebook_cpu: 1
    notebook_memory: 2Gi
    registry: 'quay.io'
    repository: 'odh-jupyterhub'
    notebook_images:
      # ODH only deploys spark-minimal-notebook by default to speed up basic deployment
      deploy_all_notebooks: False
      # If you have GPU card(s) in you cluster you can build and deploy CUDA enabled notebook images
      deploy_cuda_notebooks: False

    storage_class: ''
    db_memory: 1Gi
    jupyterhub_memory: 1Gi
    notebook_image: 's2i-minimal-notebook:3.6'
    s3_endpoint_url: ''
    #GPU mode defines a method used to spawn GPU enabled containers. Allow values are: None/empty (default), selinux, privileged
    gpu_mode: ''

    user_pvc_size: 2Gi

    # Name of the configmap that will be used when spawning a notebook for the single user
    spark_configmap_template: 'jupyterhub-spark-operator-configmap'
    # PYSPARK args to use in the notebook pod
    # These submit args should be customized for the values passed for spark_memory and spark_cpu. You'll need to account for the available memory on the spark work nodes
    spark_pyspark_submit_args: "--conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell"
    spark_pyspark_driver_python: "jupyter"
    spark_pyspark_driver_python_opts: "notebook"
    spark_home: "/opt/app-root/lib/python3.6/site-packages/pyspark/"
    spark_pythonpath: "$PYTHONPATH:/opt/app-root/lib/python3.6/site-packages/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.8.2.1-src.zip"

    # Number of master and worker nodes for the spark cluster
    spark_worker_nodes: 2
    spark_master_nodes: 1

    # Amount of cpu & memory to allocate to the each node in the cluster.
    # This value will be applied to all worker and master nodes
    spark_memory: 4Gi
    spark_cpu: 3
    # Spark image to use in the cluster
    spark_image: "quay.io/opendatahub/spark-cluster-image:spark22python36"

  # Spark operator developed by radanalyticsio - https://github.com/radanalyticsio/spark-operator
  spark-operator:
    # Deploy the ODH spark-operator role if True
    odh_deploy: true
    master_node_count: 0
    master_memory: 1Gi
    master_cpu: 1
    worker_node_count: 0
    worker_memory: 2Gi
    worker_cpu: 2

  # Seldon Delployment
  seldon:
    odh_deploy: false

  # JupyterHub deployment developed by Graham Dumpleton - https://github.com/jupyter-on-openshift/jupyterhub-quickstart
  jupyter-on-openshift:
    # Deploy the ODH jupyter-on-openshift role if True
    odh_deploy: false
    notebook_memory: 2Gi
    # Add these whitelisted environment variables from JupyterHub to the user's notebook pod
    jupyterhub_config: |
      c.KubeSpawner.env_keep = ['S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY']
    # Environment variables that will be set on the JupyterHub pod
    extra_env_vars:
      S3_ENDPOINT_URL: "http://s3.foo.com:8000"
      S3_ACCESS_KEY: "YOURS3ACCESSKEYHERE"
      S3_SECRET_KEY: "this1is2just3gibberish"

  kafka:
    odh_deploy: false
    kafka_cluster_name: odh-message-bus
    kafka_broker_replicas: 3
    kafka_zookeeper_replicas: 3

  # Deployment of Prometheus and Grafana for Monitoring of ODH
  monitoring:
    odh_deploy: true
    enable_pushgateway: false

  # Deployment of Two Sigma's BeakerX Jupyter notebook
  beakerx:
    odh_deploy: false

  # Deployment of AI Library models as SeldonDeployments.  Can only be done if Seldon is also deployed.
  ai-library:
    odh_deploy: false
 
  # Deployment of Argo 
  argo:
    odh_deploy: false

  # Deployment of Superset
  superset:
    odh_deploy: false
    # Superset Version
    version: 0.34.0
    # Create an admin user
    superset_admin:
      admin_usr: userKPJ
      admin_psw: 7ujmko0
      admin_fname: admin
      admin_lname: admin
      admin_email: admin@fab.org
    # Your App secret key
    secret_key: thisISaSECRET_1234
    # The data volume is where you would mount your SQLite file or a volume to collect any logs that are routed there
    data_volume_size: 512Mi
    # Change to an external sql backend (by default sqlite is used)
    sqlalchemy_db_uri: sqlite:////var/lib/superset/superset.db

  # Tech preview feature
  data-catalog:
    odh_deploy: false
    aws_access_key_id: changeme
    aws_secret_access_key: changeme
    s3_endpoint: s3.foo.com
    s3_port: 8080
    s3_is_secure: false

    # Parameters to create a dedicated spark cluster for Data Catalog
    spark-cluster: 
      master_node_count: 1
      master_memory: 1Gi
      master_cpu: 1
      worker_node_count: 2
      worker_memory: 2Gi
      worker_cpu: 2
      spark_cluster_name: "spark-cluster-data-catalog"
      spark_image: "quay.io/opendatahub/spark-cluster-image:spark24"

    hive-metastore:
      # Parameters for Hive Metastore Database (PostgreSQL)
      database:
        image: "registry.access.redhat.com/rhscl/postgresql-96-rhel7"
        username: changeme
        password: changeme
        memory_limit: 1Gi
        volume_capacity: 10Gi
        driver: org.postgresql.Driver

      warehouse_volume_capacity: 10Gi

    thrift-server:
      spark_cluster_port: 7077
      spark_max_cores: 6

    hue:
      # Database parameters for Hue (MySQL)
      database:
        username: changeme
        password: changeme
        root_password: changeme
        volume_capacity: 10Gi
        image: "registry.access.redhat.com/rhscl/mysql-57-rhel7"
        memory_limit: 1Gi

      # Secret key can be any random String
      hue_secret_key: changeme
