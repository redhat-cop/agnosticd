---

# TODO: Make this GPU agnostic, building it initially around Nvidia and CUDA
# TODO: Extract vars to defaults/main.yml

- name: Set CUDA related vars for all users
  ansible.builtin.blockinfile:
    path: /etc/environment
    block: |
      CUDA_HOME={{ ai_setup_ilab_cuda_home }}
      LD_LIBRARY_PATH={{ ai_setup_ilab_cuda_lib_path }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK"
    create: true

- name: Set CUDA related vars etc in .bashrc
  ansible.builtin.blockinfile:
    path: "/home/{{ ai_setup_ilab_user }}/.bashrc"
    block: |
      export CUDA_HOME={{ ai_setup_ilab_cuda_home }}
      export LD_LIBRARY_PATH={{ ai_setup_ilab_cuda_lib_path }}
      export PATH=$PATH:/usr/local/cuda/bin
      {% if not ai_setup_ilab_summit2024_mode %}
      echo "Welcome to the InstructLab VM"
      echo
      echo "- To get started see https://github.com/instructlab"
      echo "- To chat: ilab model chat"
      echo
      {% endif %}
    marker: "# {mark} ANSIBLE MANAGED BLOCK"
    create: true
  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"

- name: Setup AI developer packages
  ansible.builtin.dnf:
    name: "{{ package }}"
    state: present
  loop: "{{ ai_setup_ilab_developer_packages }}"
  loop_control:
    loop_var: package

- name: Set system default Python version, to 3.11
  ansible.builtin.alternatives:
    name: python
    link: /usr/bin/python3
    path: /usr/bin/python3.11

# TODO: Extract vars to defaults/main.yml and make more generic
- name: Setup the users InstructLab environment
  block:

    - name: Clone InstructLab repository
      ansible.builtin.git:
        repo: "{{ ai_setup_ilab_repo_url }}"
        dest: "{{ ai_setup_ilab_install_path_base }}/instructlab"
        version: "main"
        clone: true
        update: true
      register: r_git_clone_instructlab

    - name: Clone taxonomy repository if not present
      when: not ai_setup_ilab_summit2024_mode | bool
      ansible.builtin.git:
        repo: "{{ ai_setup_ilab_taxonomy_repo_url }}"
        dest: "{{ ai_setup_ilab_install_path_base }}/taxonomy"
        version: "main"
        clone: true
        update: true
      register: r_git_clone_taxonomy

  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"

# TODO: THis python setup seems hideous but it works, need to find a better way
# TODO: Remove hard paths etc

- name: Venv and iLab Python setup
  block:

    - name: Setup a Python 3.11 virtual environment
      ansible.builtin.command:
        cmd: "python3.11 -m venv /home/{{ ai_setup_ilab_user }}/instructlab/venv"
      args:
        creates: "/home/{{ ai_setup_ilab_user }}/instructlab/venv"

    - name: Upgrade venv pip
      ansible.builtin.command:
        cmd: "/home/{{ ai_setup_ilab_user }}/instructlab/venv/bin/python -m pip install --upgrade pip"

    - name: Install InstructLab package
      when: r_git_clone_instructlab.changed
      ansible.builtin.command:
        cmd: >-
          /home/{{ ai_setup_ilab_user }}/instructlab/venv/bin/pip 
          install git+https://github.com/instructlab/instructlab.git@{{ ai_setup_ilab_git_release }}

    - name: Setup llama-cpp-python with CUDA Support
      ansible.builtin.command:
        cmd: >-
            /home/{{ ai_setup_ilab_user }}/instructlab/venv/bin/pip install --force-reinstall 
            --no-cache-dir "llama-cpp-python[server]=={{ ai_setup_ilab_llama_cpp_python_version | default('0.2.75') }}"
      environment:
        CUDACXX: /usr/local/cuda-12/bin/nvcc 
        CMAKE_ARGS: "-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major" 
        FORCE_CMAKE: 1 

    - name: Initialize InstructLab non-interactively
      ansible.builtin.shell: >-
        source {{ ai_setup_ilab_install_path_base }}/instructlab/venv/bin/activate 
        && rm -rf taxonomy 
        && ilab config init --non-interactive --model-path instructlab/models/merlinite-7b-lab-Q4_K_M.gguf --taxonomy-path taxonomy 
        && ilab model download
      args:
        chdir: "{{ ai_setup_ilab_install_path_base }}/instructlab"

  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"

# ==== START: RAD Roadshow ====

- name: Download VS Code
  get_url:
    url: https://code.visualstudio.com/sha/download?build=stable&os=linux-rpm-x64
    dest: "{{ ai_setup_ilab_install_path_base }}/vscode.rpm"
    group: users
    owner: instruct

- name: Install VS Code
  shell: "sudo rpm -ivh {{ ai_setup_ilab_install_path_base }}/vscode.rpm"

- name: Download sdkman installer
  get_url:
    url: https://get.sdkman.io
    dest: "{{ ai_setup_ilab_install_path_base }}/sdkman-installer.sh"
    mode: 0755
    group: users
    owner: instruct

- name: Install sdkman
  shell: "bash {{ ai_setup_ilab_install_path_base }}/sdkman-installer.sh"
  become: true

- name: Check sdkman installation
  shell: |
    ls -al /root/
    ls -al {{ ai_setup_ilab_install_path_base }}
    ls -al /root/.sdkman/
    ls -al /root/.sdkman/bin
  register: r_result

- name: Show output
  debug:
     msg: "{{ r_result.stdout }}" 

- name: Run sdkman installer
  shell: source "/root/.sdkman/bin/sdkman-init.sh"
  become: true

- name: Install Java using sdkman
  shell: sdk install java 21.0.3-tem  # Adjust version appropriately
  register: java_install

- name: Check Java installation
  debug:
    msg: "Java {{ java_install.stdout }} installed successfully."
  when: java_install.rc == 0

- name: Download LLM for RAD (Parasol Model)
  get_url:
    url: https://huggingface.co/rh-rad-ai-roadshow/parasol-merlanite-trained-GGUF/resolve/main/parasol-model-0715-sdg400.gguf?download=true
    dest: "{{ ai_setup_ilab_install_path_base }}/instructlab/models/parasol-model.gguf"
    mode: 0644
  register: llm_download_result

- name: Check LLM download status 
  debug:
    msg: "LLM download {{ llm_download_result.msg }}"
  when: llm_download_result is defined

- name: Download a custom policy document
  get_url:
    url: https://raw.githubusercontent.com/rh-rad-ai-roadshow/parasol-insurance/main/app/src/main/resources/claims/marty-mcfly-auto.pdf
    dest: "{{ ai_setup_ilab_install_path_base }}/Documents/marty-mcfly-auto.pdf"
    mode: 0644

- name: Clone parasol-insurance repository
  git:
    repo: https://github.com/rh-rad-ai-roadshow/parasol-insurance.git
    dest: "{{ ai_setup_ilab_install_path_base }}/parasol-insurance"
    update: no  # Avoid unnecessary re-clones (adjust if needed)

- name: Build the parasol app
  shell: "{{ ai_setup_ilab_install_path_base }}/parasol-insurance/app/mvnw clean package"
  become: true

- name: Run the parasol app in background
  shell: "nohup java -jar -Dquarkus.langchain4j.openai.parasol-chat.base-url=http://localhost:8000/v1 {{ ai_setup_ilab_install_path_base }}/parasol-insurance/app/target/quarkus-app/quarkus-run.jar > ~/quarkus.out 2>&1 &"
  become: true

- name: Create settings directory for VS Code
  file:
    path: "{{ ai_setup_ilab_install_path_base }}/.config/Code/User"
    state: directory
    mode: 0755

- name: Create settings file
  ansible.builtin.copy:
    src: settings.json
    dest: "{{ ai_setup_ilab_install_path_base }}/.config/Code/User/settings.json"
    mode: u=rw,g=rw,o=
    creates: "{{ ai_setup_ilab_install_path_base }}/.config/Code/User/settings.json"

# ==== END: RAD Roadshow ====

- name: Setup AI Developer Preview with Mixtral and Granite and ai-lab images
  when: ai_setup_ilab_developer_preview | default(false) | bool
  block:

    # TODO:
    # loop the LLMs
    # Make the LLMs a list in defaults/main.yml so we can drive via AgnosticV
    # But need this out the door today!

    - name: Load Mixtral LLM via ilab
      when: not ai_setup_ilab_summit2024_mode | bool
      ansible.builtin.shell: >-
        source {{ ai_setup_ilab_install_path_base }}/instructlab/venv/bin/activate && 
        ilab model download --repository mistralai/Mixtral-8x7B-Instruct-v0.1 
        --hf-token {{ huggingface_token }}
      args:
        chdir: "{{ ai_setup_ilab_install_path_base }}/instructlab"

    - name: Load Granite LLM via ilab
      ansible.builtin.shell: >-
        source {{ ai_setup_ilab_install_path_base }}/instructlab/venv/bin/activate && 
        ilab model download  --repository ibm/granite-7b-base
        --hf-token {{ huggingface_token }}
      args:
        chdir: "{{ ai_setup_ilab_install_path_base }}/instructlab"

      #TODO: move to Defaults/main.yml, drive from AgV   

    - name: Podman Pull  
      containers.podman.podman_image:
        name: "quay.io/{{ __image }}"
        username: "{{ quay_io_username }}"
        password: "{{ quay_io_password }}"
      loop: 
        - "ai-lab/instructlab-nvidia:latest"
        - "ai-lab/vllm:latest"
        - "ai-lab/deepspeed-trainer:latest"
      loop_control:
        loop_var: __image

  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"
# End of AI Developer Preview block

- name: Add Summit 2024 files and assets  
  when: ai_setup_ilab_summit2024_mode | default(false) | bool
  block:

    - name: Create ilab directories
      ansible.builtin.file:
        path: "{{ __directory }}"
        state: directory
        owner: "{{ ai_setup_ilab_user | default('instruct') }}"
        mode: u=rwx,g=rx,o=rx
      loop:
        - "~/files"
        -  "~/instructlab/models"
      loop_control:
        loop_var: __directory

    - name: Create qna sample file
      ansible.builtin.copy:
        src: qna.yaml
        dest: "~/files/qna.yaml"
        mode: u=rw,g=rw,o=

    # - name: Load Granite LLM via ilab
    #   ansible.builtin.shell: >-
    #     source {{ ai_setup_ilab_install_path_base }}/instructlab/venv/bin/activate && 
    #     ilab model download  --repository instructlab/granite-7b-lab-GGUF 
    #     --filename=granite-7b-lab-Q4_K_M.granite-7b-lab-GGUF
    #   args:
    #     chdir: "{{ ai_setup_ilab_install_path_base }}/instructlab"

    - name: Fetch the Summit LLM via http from S3 and move to models
      ansible.builtin.get_url:
        url:  https://gpte-public.s3.amazonaws.com/llms/ggml-ilab-pretrained-Q4_K_M.gguf
        dest: "~/instructlab/models/ggml-ilab-pretrained-Q4_K_M.gguf"
        mode: u=rw,g=rw,o=

  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"

- name: Automatically activate venv at login via .bashrc
  when: not ai_setup_ilab_summit2024_mode | default(false) | bool
  ansible.builtin.lineinfile:
    path: "/home/{{ ai_setup_ilab_user }}/.bashrc"
    line: 'source instructlab/venv/bin/activate'
    create: true
    state: present
    insertafter: EOF

  become_user: "{{ ai_setup_ilab_user | default('instruct') }}"
  become: true
