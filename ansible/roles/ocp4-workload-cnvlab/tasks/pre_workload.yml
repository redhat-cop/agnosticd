---
# Implement your Pre Workload deployment tasks here
# -------------------------------------------------

- name: Configure new Python3 virtual environment
  become: true
  block:
  - name: "Copy requirements.txt for k8s-cnvlab venv"
    copy:
      src: "../files/requirements.txt"
      dest: "/tmp/requirements.txt"

  - name: "Create k8s-cnvlab venv"
    pip:
      requirements: "/tmp/requirements.txt"
      virtualenv: "/opt/virtualenvs/k8s-cnvlab"
      virtualenv_site_packages: yes
      virtualenv_command: "/usr/local/bin/virtualenv"

  - name: "Set Ansible interpreter to venv"
    set_fact:
      ansible_python_interpreter: /opt/virtualenvs/k8s-cnvlab/bin/python

- name: Install Squid and other dependencies for proxy in lab
  package:
    name:
      - squid
      - qemu-img
      - httpd
    state: present
  become: true

- name: Grab Squid config file
  get_url:
    url: https://raw.githubusercontent.com/RHFieldProductManagement/openshift-virt-labs/rhpds/configs/squid.conf
    dest: /etc/squid/squid.conf
    mode: '0755'
    owner: squid
  become: true

- name: Apply correct SELinux file context for squid.conf
  command: restorecon -v /etc/squid/squid.conf
  become: true

- name: Start Squid service
  service:
    name: squid
    state: started
    enabled: yes
  become: true

- name: Create NFS directories for PV's
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: 0777
  with_items:
    - /mnt/nfs/one
    - /mnt/nfs/two
    - /mnt/nfs/three
    - /mnt/nfs/four
  become: true

- name: Create /etc/exports file for NFS storage
  copy:
    dest: "/etc/exports"
    content: "/mnt/nfs     *(rw,sync,no_root_squash)\n"
  become: true

- name: Apply correct SELinux file context for /etc/exports
  command: restorecon -v /etc/exports
  become: true

- name: Set SEBool's for NFS Export RW
  command: setsebool -P nfs_export_all_rw 1
  become: true

- name: Set SEBool's for NFS Export RO
  command: setsebool -P nfs_export_all_ro 1
  become: true

- name: Relabel NFS Storage
  command: restorecon -Rv /mnt/nfs
  become: true

- name: Start NFS server
  service:
    name: nfs-server
    state: started
    enabled: yes
  become: true

- name: Start rpcbind service
  service:
    name: rpcbind
    state: started
    enabled: yes
  become: true

- name: Create OpenStack security group for bastion
  os_security_group:
    state: present
    name: bastion-secgroup
    description: Bastion Customised SecGroup

- name: Configure security group ACL tcp/111 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: tcp
    port_range_min: 111
    port_range_max: 111
    remote_ip_prefix: 0.0.0.0/0

- name: Configure security group ACL tcp/2049 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: tcp
    port_range_min: 2049
    port_range_max: 2049
    remote_ip_prefix: 0.0.0.0/0

- name: Configure security group ACL tcp/20048 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: tcp
    port_range_min: 20048
    port_range_max: 20048
    remote_ip_prefix: 0.0.0.0/0

- name: Configure security group ACL udp/111 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: udp
    port_range_min: 111
    port_range_max: 111
    remote_ip_prefix: 0.0.0.0/0

- name: Configure security group ACL udp/2049 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: udp
    port_range_min: 2049
    port_range_max: 2049
    remote_ip_prefix: 0.0.0.0/0

- name: Configure security group ACL udp/20048 for bastion secgroup
  os_security_group_rule:
    security_group: bastion-secgroup
    protocol: udp
    port_range_min: 20048
    port_range_max: 20048
    remote_ip_prefix: 0.0.0.0/0

- name: Add new sec-group to bastion host
  shell: openstack server add security group bastion bastion-secgroup
  # ignoring errors here as there's no way to do this with Ansible module
  ignore_errors: yes

- name: Create OpenStack port01 for CNV VM's that need DHCP
  os_port:
    state: present
    name: cnv-port01
    mac_address: de:ad:be:ef:00:01
    port_security_enabled: no
    network: "{{ guid }}-ocp-network"
    fixed_ips:
      - ip_address: 192.168.47.5

- name: Create OpenStack port02 for CNV VM's that need DHCP
  os_port:
    state: present
    name: cnv-port02
    mac_address: de:ad:be:ef:00:02
    port_security_enabled: no
    network: "{{ guid }}-ocp-network"
    fixed_ips:
      - ip_address: 192.168.47.6

- name: Create OpenStack port03 for CNV VM's that need DHCP
  os_port:
    state: present
    name: cnv-port03
    mac_address: de:ad:be:ef:00:03
    port_security_enabled: no
    network: "{{ guid }}-ocp-network"
    fixed_ips:
      - ip_address: 192.168.47.7

- name: Create OpenStack port04 for CNV VM's that need DHCP
  os_port:
    state: present
    name: cnv-port04
    mac_address: de:ad:be:ef:00:04
    port_security_enabled: no
    network: "{{ guid }}-ocp-network"
    fixed_ips:
      - ip_address: 192.168.47.8

- name: Get list of OpenShift workers
  k8s_info:
    kind: Node
    api_version: v1
    label_selectors:
      - "node-role.kubernetes.io/worker"
  register: worker_list

- name: Create OpenStack port for second network on workers
  os_port:
    state: present
    name: "port2-{{ item.metadata.name }}"
    port_security_enabled: no
    network: "{{ guid }}-ocp-network"
  with_items: '{{ worker_list.resources }}'

- name: Add second NIC to each of the workers for direct L2 network
  shell: openstack server add port {{ item.metadata.name }} port2-{{ item.metadata.name }}
  with_items: '{{ worker_list.resources }}'
  # Ignore errors here as there's no module to handle this cleanly
  ignore_errors: yes

# For RHPDS deployment (onto a shared cluster) set
# workload_shared_deployment to True
# (in the deploy script or AgnosticV configuration)
- name: pre_workload tasks complete
  debug:
    msg: "Pre workload deployment setup completed successfully"
  when:
  - not silent|bool
  - workload_shared_deployment|default(False)
