= ocp-workload-odo-workshop - odo summit lab environment

== Role overview

* This is a simple role that does the following:
** Tasks: link:./tasks/pre_workload.yml[pre_workload.yml] - Sets up an
 environment for the workload deployment
*** Copies ~/.kube/config to a temp directory and sets the KUBECONFIG env variable.
*** Debug task will print out: `pre_workload Tasks Complete`

** Tasks: link:./tasks/workload.yml[workload.yml] - Used to deploy the 
 odo lab environment.
*** Creates a new project and a deployment for the learning portal.
*** Debug task will print out: `workload Tasks Complete`

** Tasks: link:./tasks/post_workload.yml[post_workload.yml] - Used to
 configure the workload after deployment
*** Removes the temp .kube/config file.
*** Debug task will print out: `post_workload Tasks Complete`

** Tasks: link:./tasks/pre_remove_workload.yml[pre_remove_workload.yml] - Sets up an
 environment for the workload removal
*** Copies ~/.kube/config to a temp directory and sets the KUBECONFIG env variable.
*** Debug task will print out: `pre_workload Removal Tasks Complete`

** Tasks: link:./tasks/remove_workload.yml[remove_workload.yml] - Used to clean up the environment and delete all resources.
*** Deletes resources and projects that were created.
*** Debug task will print out: `workload Removal Tasks Complete`

** Tasks: link:./tasks/post_remove_workload.yml[post_remove_workload.yml] - Used to
 configure the workload after deployment
*** Removes the temp .kube/config file.
*** Debug task will print out: `post_workload Removal Tasks Complete`

== Set up your Ansible inventory file

* You will need to create an Ansible inventory file to define your connection
 method to your host (Master/Bastion with OC command)

* You can also use the command line to define the hosts directly if your `ssh`
 configuration is set to connect to the host correctly

* You can also use the command line to use localhost or if your cluster is
 already authenticated and configured in your `oc` configuration
[source, ini]

.example inventory file
----
[gptehosts:vars]
ansible_ssh_private_key_file=~/.ssh/keytoyourhost.pem
ansible_user=ec2-user

[gptehosts:children]
openshift

[openshift]
bastion.cluster1.openshift.opentlc.com
bastion.cluster2.openshift.opentlc.com
bastion.cluster3.openshift.opentlc.com ansible_ssh_host=ec2-11-111-111-11.us-west-2.compute.amazonaws.com
bastion.cluster4.openshift.opentlc.com


[dev]
bastion.cluster1.openshift.opentlc.com
bastion.cluster2.openshift.opentlc.com


[prod]
bastion.cluster3.openshift.opentlc.com
bastion.cluster4.openshift.opentlc.com
----


== Review the defaults variable file

* This file link:./defaults/main.yml[./defaults/main.yml] contains all the variables you
 need to define to control the deployment of your workload.

* You can modify any of these default values by adding
`-e"variable_name=variable_value"` to the command line

=== Deploy Workload on OpenShift Cluster from an existing playbook:

[source,yaml]
----
- name: Deploy a workload role on a master host
  hosts: all
  become: true
  gather_facts: False
  tags:
    - step007
  roles:
    - { role: "{{ocp_workload}}", when: 'ocp_workload is defined' }

----
NOTE: You might want to change `hosts: all` to fit your requirements

=== Common configuration to run these playbooks
You should have these environment variables defined/exported in your system in order
to run these playbooks.

----
HOST_GUID=jkleinert-007f
DOMAIN="$HOST_GUID.openshiftworkshop.com"
TARGET_HOST="bastion.$DOMAIN"
MASTER_HOSTNAME="master.$DOMAIN"
APPS_DOMAIN="apps.$DOMAIN"
OCP_USERNAME="opentlc-mgr"
SSH_USER="ec2-user"
SSH_PRIVATE_KEY="ocp-workshop.pem"
GUID=jkleinert-007f
# WORKLOAD SPECIFICS
WORKSHOP_PROJECT="workshop"
WORKLOAD="ocp-workload-odo-workshop"
----

=== Deploy a Workload with the `ocp-workload` playbook [Mostly for testing]
----
WORKLOAD="ocp-workload-odo-workshop"

# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
                -e"ansible_ssh_private_key_file=~/.ssh/${SSH_PRIVATE_KEY}" \
                -e"ansible_ssh_user=${SSH_USER}" \
                -e"ANSIBLE_REPO_PATH=`pwd`" \
                -e"ocp_username=${OCP_USERNAME}" \
                -e"ocp_workload=${WORKLOAD}" \
                -e"guid=${GUID}" \
                -e"odo_project_name=${WORKSHOP_PROJECT}" \
                -e"ACTION=create"
----

=== To Delete an environment
----
WORKLOAD="ocp-workload-odo-workshop"

# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
                -e"ansible_ssh_private_key_file=~/.ssh/${SSH_PRIVATE_KEY}" \
                -e"ansible_ssh_user=${SSH_USER}" \
                -e"ANSIBLE_REPO_PATH=`pwd`" \
                -e"ocp_username=${OCP_USERNAME}" \
                -e"ocp_workload=${WORKLOAD}" \
                -e"guid=${GUID}" \
                -e"odo_project_name=${WORKSHOP_PROJECT}" \
                -e"ACTION=remove"
----

=== Additional information

== Additional configuration
You can alter the defaults provided when running your ansible role by
providing the name of the variable via *ENV* variable (with -e).

The values that can be set (and the defaults) are:

----
become_override: false # set to true if your SSH_USER is something other than opentlc-mgr, e.g. ec2-user 
ocp_username: opentlc-mgr

odo_project_name: workshop
odo_application_name: odo


----
